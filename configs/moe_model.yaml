# Mixture of Experts (MoE) Model Configuration
# ~500M total parameters, ~100M active per forward pass
#
# MODEL TYPE: Sparse Mixture of Experts decoder-only transformer
# ARCHITECTURE: 16 layers × 1024 hidden × 16 heads
# MoE CONFIG: 8 experts per layer, Top-2 routing, every other layer
# CONTEXT: 2048 tokens
#
# WHEN TO USE:
# - Learning modern sparse MoE architectures (Mixtral, GPT-4 style)
# - Research on expert specialization and routing
# - Maximum model capacity within 32GB VRAM constraint
# - Exploring expert load balancing and utilization
# - Educational projects on cutting-edge LLM techniques
#
# EXPECTED TRAINING TIME: 4-5 days on consumer hardware (32GB VRAM)
# HARDWARE: 32GB VRAM required (CUDA or MPS)
# OUTPUT QUALITY: Potentially higher quality than base_model with same active params
#
# IDEAL FOR: Advanced users exploring sparse models and expert routing
# NOTE: Primary goal is educational - learning MoE concepts, not just story quality

model:
  config_name: "moe"
  vocab_size: 50257  # Will be updated after tokenizer training
  max_seq_length: 2048
  hidden_size: 1024
  num_layers: 16
  num_attention_heads: 16
  intermediate_size: 4096
  attention_dropout: 0.1
  hidden_dropout: 0.1
  positional_encoding: "rope"
  activation: "gelu"
  gradient_checkpointing: true

  # MoE specific
  use_moe: true
  num_experts: 8
  top_k_experts: 2
  moe_frequency: 2  # Apply MoE every other layer
  expert_capacity_factor: 1.25
  load_balancing_loss_weight: 0.01
  router_z_loss_weight: 0.001

training:
  # Data
  train_data_path: "data/processed/train.txt"
  val_data_path: "data/processed/val.txt"
  tokenizer_path: "data/tokenizers/storyteller-tokenizer"  # Path to trained tokenizer
  use_cached_dataset: true  # Use StoryDatasetPreloaded with caching
  cache_dir: "data/cache"  # Directory for cached tokenized data

  # Optimization
  batch_size: 4  # Smaller batch for MoE
  gradient_accumulation_steps: 8  # Effective batch size: 32
  learning_rate: 2.0e-4  # Slightly lower for MoE stability
  weight_decay: 0.1
  max_grad_norm: 1.0

  # Schedule
  num_epochs: 15
  warmup_steps: 3000  # Longer warmup for MoE
  lr_scheduler: "cosine"

  # Mixed precision
  use_amp: true
  amp_dtype: "bfloat16"

  # Checkpointing
  save_dir: "checkpoints/moe_model"
  save_every_n_steps: 5000
  keep_last_n_checkpoints: 3

  # Logging
  log_every_n_steps: 100
  eval_every_n_steps: 1000
  use_mlflow: true
  mlflow_experiment_name: "storyteller"
  mlflow_run_name: "moe_model"
  mlflow_tracking_uri: "http://localhost:8080"
  mlflow_log_system_metrics: true  # Log CPU, GPU, memory metrics
  log_expert_stats: true  # Log expert utilization

  # Evaluation metrics (Phase 1)
  evaluation:
    num_eval_samples: 50  # Number of stories to generate during evaluation
    eval_max_length: 512  # Maximum length of generated stories
    eval_temperature: 1.0  # Sampling temperature for evaluation
    eval_top_k: 50  # Top-k sampling parameter
    eval_top_p: 0.95  # Top-p (nucleus) sampling parameter

  # Hardware
  # Device options: "smart", "mps", "cuda", "cuda:0", "cuda:1", "cpu"
  # "smart" will intelligently select: MPS > available CUDA GPU > CPU
  device: "smart"
  num_workers: 4
  pin_memory: true

generation:
  max_new_tokens: 512
  temperature: 0.9
  top_p: 0.95
  top_k: 50
  repetition_penalty: 1.1
