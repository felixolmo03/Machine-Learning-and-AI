{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Module 1: Data Preparation and Tokenization\n",
    "\n",
    "Welcome to the first module of the Storyteller training series! In this notebook, we'll explore how to prepare data for training a language model.\n",
    "\n",
    "## Learning Objectives\n",
    "\n",
    "By the end of this notebook, you will understand:\n",
    "1. How to download and explore story datasets\n",
    "2. Text preprocessing techniques for language models\n",
    "3. How tokenization works (Byte-Pair Encoding)\n",
    "4. How to train a custom tokenizer\n",
    "5. How to analyze and visualize text data\n",
    "\n",
    "## Why Data Preparation Matters\n",
    "\n",
    "The quality of your training data directly impacts model performance:\n",
    "- **Garbage in, garbage out**: Poor data leads to poor models\n",
    "- **Tokenization**: Affects vocabulary size, compression, and efficiency\n",
    "- **Preprocessing**: Removes noise and normalizes text\n",
    "- **Statistics**: Understanding your data helps debug training issues"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "from pathlib import Path\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from collections import Counter\n",
    "\n",
    "# Add project to path\n",
    "project_root = Path.cwd().parent\n",
    "sys.path.insert(0, str(project_root / \"src\"))\n",
    "\n",
    "# Set plotting style\n",
    "sns.set_style(\"whitegrid\")\n",
    "plt.rcParams[\"figure.figsize\"] = (12, 6)\n",
    "\n",
    "print(\"Environment ready!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 1: Understanding Our Datasets\n",
    "\n",
    "We'll use two datasets for training:\n",
    "\n",
    "### 1. TinyStories\n",
    "- **Size**: ~2M short stories\n",
    "- **Style**: Simple, child-friendly narratives\n",
    "- **Vocabulary**: Limited, easy to learn\n",
    "- **Good for**: Quick experimentation, testing\n",
    "\n",
    "### 2. WritingPrompts\n",
    "- **Size**: ~300K creative stories\n",
    "- **Style**: Diverse, creative writing\n",
    "- **Vocabulary**: Rich, complex\n",
    "- **Good for**: Better quality, more variety"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check if data exists\n",
    "data_dir = project_root / \"data\" / \"raw\"\n",
    "\n",
    "tinystories_path = data_dir / \"tinystories.txt\"\n",
    "writingprompts_path = data_dir / \"writingprompts.txt\"\n",
    "\n",
    "print(\"Dataset Status:\")\n",
    "print(f\"  TinyStories: {'✓ Found' if tinystories_path.exists() else '✗ Not found'}\")\n",
    "print(\n",
    "    f\"  WritingPrompts: {'✓ Found' if writingprompts_path.exists() else '✗ Not found'}\"\n",
    ")\n",
    "\n",
    "if not tinystories_path.exists() and not writingprompts_path.exists():\n",
    "    print(\"\\nTo download datasets, run:\")\n",
    "    print(\"  storyteller-download --datasets tinystories writingprompts\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 2: Loading and Exploring Data\n",
    "\n",
    "Let's load some stories and analyze them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_sample_stories(file_path, num_samples=10):\n",
    "    \"\"\"Load a sample of stories from a file.\"\"\"\n",
    "    stories = []\n",
    "\n",
    "    if not Path(file_path).exists():\n",
    "        print(f\"File not found: {file_path}\")\n",
    "        return stories\n",
    "\n",
    "    with open(file_path, \"r\", encoding=\"utf-8\") as f:\n",
    "        for i, line in enumerate(f):\n",
    "            if i >= num_samples:\n",
    "                break\n",
    "            stories.append(line.strip())\n",
    "\n",
    "    return stories\n",
    "\n",
    "\n",
    "# Load samples\n",
    "if tinystories_path.exists():\n",
    "    sample_stories = load_sample_stories(tinystories_path, 5)\n",
    "\n",
    "    print(\"Sample Stories from TinyStories:\")\n",
    "    print(\"=\" * 70)\n",
    "    for i, story in enumerate(sample_stories, 1):\n",
    "        print(f\"\\nStory {i}:\")\n",
    "        print(story[:200] + \"...\" if len(story) > 200 else story)\n",
    "        print(\"-\" * 70)\n",
    "else:\n",
    "    print(\"Using example story for demonstration...\")\n",
    "    sample_stories = [\n",
    "        \"Once upon a time, there was a little girl named Lily. She loved to play outside.\"\n",
    "    ]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 3: Text Statistics\n",
    "\n",
    "Understanding your data is crucial. Let's compute basic statistics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def analyze_text_file(file_path, max_stories=10000):\n",
    "    \"\"\"Analyze a text file and return statistics.\"\"\"\n",
    "    if not Path(file_path).exists():\n",
    "        return None\n",
    "\n",
    "    story_lengths = []\n",
    "    total_chars = 0\n",
    "    word_counter = Counter()\n",
    "    char_counter = Counter()\n",
    "\n",
    "    with open(file_path, \"r\", encoding=\"utf-8\") as f:\n",
    "        for i, line in enumerate(f):\n",
    "            if i >= max_stories:\n",
    "                break\n",
    "\n",
    "            story = line.strip()\n",
    "            story_lengths.append(len(story))\n",
    "            total_chars += len(story)\n",
    "\n",
    "            # Count words\n",
    "            words = story.lower().split()\n",
    "            word_counter.update(words)\n",
    "\n",
    "            # Count characters\n",
    "            char_counter.update(story)\n",
    "\n",
    "    return {\n",
    "        \"num_stories\": len(story_lengths),\n",
    "        \"story_lengths\": story_lengths,\n",
    "        \"total_chars\": total_chars,\n",
    "        \"avg_length\": np.mean(story_lengths),\n",
    "        \"median_length\": np.median(story_lengths),\n",
    "        \"vocab_size\": len(word_counter),\n",
    "        \"total_words\": sum(word_counter.values()),\n",
    "        \"unique_chars\": len(char_counter),\n",
    "        \"most_common_words\": word_counter.most_common(20),\n",
    "    }\n",
    "\n",
    "\n",
    "# Analyze datasets\n",
    "if tinystories_path.exists():\n",
    "    print(\"Analyzing TinyStories (this may take a minute)...\")\n",
    "    stats = analyze_text_file(tinystories_path)\n",
    "\n",
    "    if stats:\n",
    "        print(\"\\nTinyStories Statistics:\")\n",
    "        print(f\"  Stories analyzed: {stats['num_stories']:,}\")\n",
    "        print(f\"  Total characters: {stats['total_chars']:,}\")\n",
    "        print(f\"  Average story length: {stats['avg_length']:.0f} chars\")\n",
    "        print(f\"  Median story length: {stats['median_length']:.0f} chars\")\n",
    "        print(f\"  Vocabulary size: {stats['vocab_size']:,} unique words\")\n",
    "        print(f\"  Total words: {stats['total_words']:,}\")\n",
    "        print(f\"  Unique characters: {stats['unique_chars']}\")\n",
    "\n",
    "        print(\"\\n  Most common words:\")\n",
    "        for word, count in stats[\"most_common_words\"][:10]:\n",
    "            print(f\"    {word}: {count:,}\")\n",
    "else:\n",
    "    print(\"Dataset not available. Using example data...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 4: Visualizing Story Lengths\n",
    "\n",
    "Understanding the distribution of story lengths helps us choose appropriate sequence lengths for training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if tinystories_path.exists() and stats:\n",
    "    fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "    # Histogram\n",
    "    ax1.hist(stats[\"story_lengths\"], bins=50, alpha=0.7, edgecolor=\"black\")\n",
    "    ax1.axvline(\n",
    "        stats[\"avg_length\"],\n",
    "        color=\"red\",\n",
    "        linestyle=\"--\",\n",
    "        label=f\"Mean: {stats['avg_length']:.0f}\",\n",
    "    )\n",
    "    ax1.axvline(\n",
    "        stats[\"median_length\"],\n",
    "        color=\"green\",\n",
    "        linestyle=\"--\",\n",
    "        label=f\"Median: {stats['median_length']:.0f}\",\n",
    "    )\n",
    "    ax1.set_xlabel(\"Story Length (characters)\")\n",
    "    ax1.set_ylabel(\"Count\")\n",
    "    ax1.set_title(\"Distribution of Story Lengths\")\n",
    "    ax1.legend()\n",
    "    ax1.grid(alpha=0.3)\n",
    "\n",
    "    # Cumulative distribution\n",
    "    sorted_lengths = np.sort(stats[\"story_lengths\"])\n",
    "    cumulative = np.arange(1, len(sorted_lengths) + 1) / len(sorted_lengths)\n",
    "    ax2.plot(sorted_lengths, cumulative, linewidth=2)\n",
    "\n",
    "    # Mark percentiles\n",
    "    for percentile in [50, 90, 95, 99]:\n",
    "        length = np.percentile(stats[\"story_lengths\"], percentile)\n",
    "        ax2.axvline(length, color=\"red\", linestyle=\":\", alpha=0.5)\n",
    "        ax2.text(length, 0.5, f\"{percentile}%\", rotation=90, va=\"center\")\n",
    "\n",
    "    ax2.set_xlabel(\"Story Length (characters)\")\n",
    "    ax2.set_ylabel(\"Cumulative Proportion\")\n",
    "    ax2.set_title(\"Cumulative Distribution of Story Lengths\")\n",
    "    ax2.grid(alpha=0.3)\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "    print(\"\\nPercentiles:\")\n",
    "    for p in [50, 75, 90, 95, 99]:\n",
    "        print(\n",
    "            f\"  {p}th percentile: {np.percentile(stats['story_lengths'], p):.0f} chars\"\n",
    "        )\n",
    "else:\n",
    "    print(\"Visualization requires dataset to be downloaded.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 5: Understanding Tokenization\n",
    "\n",
    "### What is Tokenization?\n",
    "\n",
    "Tokenization converts text into numerical sequences that models can process.\n",
    "\n",
    "**Example:**\n",
    "```\n",
    "Text: \"Hello world!\"\n",
    "Tokens: [\"Hello\", \" world\", \"!\"]\n",
    "Token IDs: [284, 995, 0]\n",
    "```\n",
    "\n",
    "### Why Byte-Pair Encoding (BPE)?\n",
    "\n",
    "BPE is a subword tokenization algorithm that:\n",
    "1. Starts with character-level tokens\n",
    "2. Iteratively merges most frequent pairs\n",
    "3. Balances vocabulary size and token sequence length\n",
    "\n",
    "**Advantages:**\n",
    "- Handles unknown words (can represent any text)\n",
    "- Efficient vocabulary size\n",
    "- Good compression ratio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Simple character-level tokenization example\n",
    "def char_tokenize(text):\n",
    "    \"\"\"Simple character-level tokenization.\"\"\"\n",
    "    return list(text)\n",
    "\n",
    "\n",
    "# Simple word-level tokenization example\n",
    "def word_tokenize(text):\n",
    "    \"\"\"Simple word-level tokenization.\"\"\"\n",
    "    return text.split()\n",
    "\n",
    "\n",
    "example_text = \"Hello world! This is tokenization.\"\n",
    "\n",
    "print(\"Example Text:\", example_text)\n",
    "print()\n",
    "print(\"Character-level tokens:\")\n",
    "char_tokens = char_tokenize(example_text)\n",
    "print(f\"  Tokens: {char_tokens}\")\n",
    "print(f\"  Count: {len(char_tokens)}\")\n",
    "print()\n",
    "print(\"Word-level tokens:\")\n",
    "word_tokens = word_tokenize(example_text)\n",
    "print(f\"  Tokens: {word_tokens}\")\n",
    "print(f\"  Count: {len(word_tokens)}\")\n",
    "print()\n",
    "print(\"BPE sits in between - subword level!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 6: Training a BPE Tokenizer\n",
    "\n",
    "Let's train a custom BPE tokenizer using the HuggingFace tokenizers library."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tokenizers import Tokenizer, models, trainers, pre_tokenizers, decoders\n",
    "\n",
    "\n",
    "def train_bpe_tokenizer(files, vocab_size=10000, min_frequency=2):\n",
    "    \"\"\"\n",
    "    Train a BPE tokenizer.\n",
    "\n",
    "    Args:\n",
    "        files: List of text files to train on\n",
    "        vocab_size: Target vocabulary size\n",
    "        min_frequency: Minimum frequency for a token to be included\n",
    "    \"\"\"\n",
    "    # Initialize tokenizer with BPE model\n",
    "    tokenizer = Tokenizer(models.BPE())\n",
    "\n",
    "    # Use whitespace pre-tokenizer\n",
    "    tokenizer.pre_tokenizer = pre_tokenizers.Whitespace()\n",
    "\n",
    "    # Use BPE decoder\n",
    "    tokenizer.decoder = decoders.BPEDecoder()\n",
    "\n",
    "    # Configure trainer\n",
    "    trainer = trainers.BpeTrainer(\n",
    "        vocab_size=vocab_size,\n",
    "        min_frequency=min_frequency,\n",
    "        special_tokens=[\"<pad>\", \"<unk>\", \"<s>\", \"</s>\"],\n",
    "        show_progress=True,\n",
    "    )\n",
    "\n",
    "    # Train\n",
    "    print(f\"Training BPE tokenizer with vocab_size={vocab_size}...\")\n",
    "    tokenizer.train(files, trainer)\n",
    "\n",
    "    print(\"✓ Tokenizer trained!\")\n",
    "    print(f\"  Vocabulary size: {tokenizer.get_vocab_size()}\")\n",
    "\n",
    "    return tokenizer\n",
    "\n",
    "\n",
    "# Train tokenizer if data exists\n",
    "if tinystories_path.exists():\n",
    "    # For demo, train on small sample\n",
    "    print(\"Training tokenizer (this may take a minute)...\")\n",
    "    tokenizer = train_bpe_tokenizer(\n",
    "        files=[str(tinystories_path)],\n",
    "        vocab_size=5000,\n",
    "        min_frequency=2,\n",
    "    )\n",
    "else:\n",
    "    print(\"Dataset not available. Skipping tokenizer training.\")\n",
    "    tokenizer = None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 7: Testing the Tokenizer\n",
    "\n",
    "Let's see how our tokenizer performs on sample text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if tokenizer:\n",
    "    test_sentences = [\n",
    "        \"Once upon a time, there was a little girl.\",\n",
    "        \"The quick brown fox jumps over the lazy dog.\",\n",
    "        \"Hello, how are you today?\",\n",
    "        \"Tokenization is the process of breaking text into tokens.\",\n",
    "    ]\n",
    "\n",
    "    print(\"Tokenization Examples:\")\n",
    "    print(\"=\" * 70)\n",
    "\n",
    "    for sentence in test_sentences:\n",
    "        encoding = tokenizer.encode(sentence)\n",
    "\n",
    "        print(f\"\\nOriginal: {sentence}\")\n",
    "        print(f\"Tokens: {encoding.tokens}\")\n",
    "        print(f\"Token IDs: {encoding.ids}\")\n",
    "        print(f\"Number of tokens: {len(encoding.tokens)}\")\n",
    "        print(\n",
    "            f\"Compression ratio: {len(sentence) / len(encoding.tokens):.2f} chars/token\"\n",
    "        )\n",
    "        print(\"-\" * 70)\n",
    "else:\n",
    "    print(\"Tokenizer not trained. Please download dataset first.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 8: Vocabulary Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if tokenizer:\n",
    "    vocab = tokenizer.get_vocab()\n",
    "\n",
    "    print(\"Vocabulary Statistics:\")\n",
    "    print(f\"  Total vocabulary size: {len(vocab)}\")\n",
    "\n",
    "    # Sample vocabulary items\n",
    "    print(\"\\n  Sample tokens:\")\n",
    "    sample_items = list(vocab.items())[:20]\n",
    "    for token, idx in sample_items:\n",
    "        print(f\"    '{token}': {idx}\")\n",
    "\n",
    "    # Analyze token lengths\n",
    "    token_lengths = [len(token) for token in vocab.keys()]\n",
    "\n",
    "    print(\"\\n  Token length statistics:\")\n",
    "    print(f\"    Min: {min(token_lengths)} chars\")\n",
    "    print(f\"    Max: {max(token_lengths)} chars\")\n",
    "    print(f\"    Average: {np.mean(token_lengths):.2f} chars\")\n",
    "    print(f\"    Median: {np.median(token_lengths):.0f} chars\")\n",
    "\n",
    "    # Visualize token lengths\n",
    "    plt.figure(figsize=(10, 5))\n",
    "    plt.hist(token_lengths, bins=30, alpha=0.7, edgecolor=\"black\")\n",
    "    plt.xlabel(\"Token Length (characters)\")\n",
    "    plt.ylabel(\"Count\")\n",
    "    plt.title(\"Distribution of Token Lengths in Vocabulary\")\n",
    "    plt.grid(alpha=0.3)\n",
    "    plt.show()\n",
    "else:\n",
    "    print(\"Tokenizer not available.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 9: Saving the Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if tokenizer:\n",
    "    # Create directory\n",
    "    tokenizer_dir = project_root / \"models\" / \"tokenizer\"\n",
    "    tokenizer_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "    # Save tokenizer\n",
    "    tokenizer_path = tokenizer_dir / \"demo_tokenizer.json\"\n",
    "    tokenizer.save(str(tokenizer_path))\n",
    "\n",
    "    print(f\"✓ Tokenizer saved to: {tokenizer_path}\")\n",
    "    print(\"\\nTo use this tokenizer in training:\")\n",
    "    print(\"  from tokenizers import Tokenizer\")\n",
    "    print(f\"  tokenizer = Tokenizer.from_file('{tokenizer_path}')\")\n",
    "else:\n",
    "    print(\"No tokenizer to save.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "In this notebook, you learned:\n",
    "\n",
    "1. **Dataset Understanding**:\n",
    "   - TinyStories and WritingPrompts datasets\n",
    "   - Loading and exploring text data\n",
    "   - Computing text statistics\n",
    "\n",
    "2. **Text Analysis**:\n",
    "   - Story length distributions\n",
    "   - Vocabulary statistics\n",
    "   - Character and word frequencies\n",
    "\n",
    "3. **Tokenization**:\n",
    "   - Different tokenization strategies\n",
    "   - Byte-Pair Encoding (BPE)\n",
    "   - Training a custom tokenizer\n",
    "   - Analyzing compression ratios\n",
    "\n",
    "4. **Practical Skills**:\n",
    "   - Using HuggingFace tokenizers library\n",
    "   - Visualizing data distributions\n",
    "   - Saving and loading tokenizers\n",
    "\n",
    "### Next Steps\n",
    "\n",
    "In the next notebook, we'll dive into transformer architectures and understand the attention mechanism that powers modern language models!\n",
    "\n",
    "### Key Takeaways\n",
    "\n",
    "- **Good tokenization is crucial** for model efficiency and performance\n",
    "- **BPE balances** vocabulary size and sequence length\n",
    "- **Understanding your data** helps you make better modeling decisions\n",
    "- **Compression ratio** affects training speed and memory usage"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise\n",
    "\n",
    "Try these experiments:\n",
    "\n",
    "1. **Different Vocabulary Sizes**: Train tokenizers with 1000, 5000, and 10000 vocab size. How does compression change?\n",
    "\n",
    "2. **Compare Datasets**: If you have both TinyStories and WritingPrompts, train separate tokenizers and compare their vocabularies.\n",
    "\n",
    "3. **Custom Text**: Tokenize your own creative writing. How well does the tokenizer handle it?\n",
    "\n",
    "4. **Token Frequency**: Analyze which tokens appear most frequently in the encoded data."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
