{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Module 3: Building Mixture of Experts (MoE)\n",
    "\n",
    "In this notebook, we'll dive deep into **Mixture of Experts** (MoE) - a powerful technique for scaling language models efficiently.\n",
    "\n",
    "## Learning Objectives\n",
    "\n",
    "By the end of this notebook, you will:\n",
    "1. Understand the motivation behind MoE architectures\n",
    "2. Learn how expert routing works (Top-K gating)\n",
    "3. Implement a complete MoE layer from scratch\n",
    "4. Explore load balancing and routing strategies\n",
    "5. Compare MoE vs dense models in terms of parameters and compute\n",
    "6. Build a complete MoE transformer\n",
    "\n",
    "## What You'll Learn\n",
    "\n",
    "- **Sparse vs Dense Models**: Why not all parameters need to be active\n",
    "- **Expert Routing**: How to intelligently route tokens to specialists\n",
    "- **Load Balancing**: Ensuring experts are used efficiently\n",
    "- **Scaling Laws**: Get more capacity without proportional compute cost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from typing import Tuple, Dict\n",
    "\n",
    "# Set random seeds\n",
    "torch.manual_seed(42)\n",
    "np.random.seed(42)\n",
    "\n",
    "# Configure plotting\n",
    "sns.set_style(\"whitegrid\")\n",
    "plt.rcParams[\"figure.figsize\"] = (12, 6)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 1: Why Mixture of Experts?\n",
    "\n",
    "### The Problem: Scaling Language Models\n",
    "\n",
    "Larger models generally perform better, but they're expensive:\n",
    "- **GPT-3**: 175B parameters, huge compute cost\n",
    "- **Training**: Requires massive GPU clusters\n",
    "- **Inference**: Slow and costly\n",
    "\n",
    "### The Insight: Conditional Computation\n",
    "\n",
    "Not all parameters need to be active for every input!\n",
    "\n",
    "**Dense Model**:\n",
    "```\n",
    "Every token → All parameters → Output\n",
    "500M params active per token\n",
    "```\n",
    "\n",
    "**MoE Model**:\n",
    "```\n",
    "Every token → Route to 2 of 8 experts → Output  \n",
    "Total: 500M params, Active: ~125M params per token\n",
    "```\n",
    "\n",
    "### Benefits:\n",
    "\n",
    "1. **More parameters, same compute**: 4x model capacity, 2x compute\n",
    "2. **Specialization**: Different experts learn different patterns\n",
    "3. **Better scaling**: Sub-linear compute growth with capacity"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualizing the Concept"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare parameter count vs active parameters\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "# Model sizes\n",
    "model_sizes = [\"100M\", \"500M\", \"1B\", \"5B\", \"10B\"]\n",
    "x_pos = np.arange(len(model_sizes))\n",
    "\n",
    "# Dense model: all parameters active\n",
    "dense_total = [100, 500, 1000, 5000, 10000]\n",
    "dense_active = dense_total.copy()\n",
    "\n",
    "# MoE model: only fraction active (assume 25% with 8 experts, top-2)\n",
    "moe_total = [100, 500, 1000, 5000, 10000]\n",
    "moe_active = [25, 125, 250, 1250, 2500]\n",
    "\n",
    "# Plot 1: Total vs Active Parameters\n",
    "width = 0.35\n",
    "ax1.bar(x_pos - width / 2, dense_active, width, label=\"Dense (Active)\", alpha=0.8)\n",
    "ax1.bar(x_pos + width / 2, moe_active, width, label=\"MoE (Active)\", alpha=0.8)\n",
    "ax1.set_ylabel(\"Active Parameters (Millions)\")\n",
    "ax1.set_xlabel(\"Model Size\")\n",
    "ax1.set_title(\"Active Parameters: Dense vs MoE\")\n",
    "ax1.set_xticks(x_pos)\n",
    "ax1.set_xticklabels(model_sizes)\n",
    "ax1.legend()\n",
    "ax1.grid(axis=\"y\", alpha=0.3)\n",
    "\n",
    "# Plot 2: Compute efficiency (FLOPs per forward pass)\n",
    "# Assume compute proportional to active parameters\n",
    "ax2.plot(\n",
    "    dense_total, dense_active, \"o-\", label=\"Dense Model\", linewidth=2, markersize=8\n",
    ")\n",
    "ax2.plot(moe_total, moe_active, \"s-\", label=\"MoE Model\", linewidth=2, markersize=8)\n",
    "ax2.set_xlabel(\"Total Parameters (Millions)\")\n",
    "ax2.set_ylabel(\"Compute Cost (Arbitrary Units)\")\n",
    "ax2.set_title(\"Compute Scaling: Dense vs MoE\")\n",
    "ax2.legend()\n",
    "ax2.grid(alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"Key Insight: MoE models achieve sub-linear compute growth!\")\n",
    "print(\"At 10B parameters: Dense uses 10B, MoE uses only 2.5B per forward pass\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 2: Expert Routing - The Core Mechanism\n",
    "\n",
    "### How Do We Choose Which Experts to Use?\n",
    "\n",
    "The **router** (also called **gating network**) decides which experts process each token.\n",
    "\n",
    "### Top-K Routing Algorithm:\n",
    "\n",
    "1. **Router Network**: Small neural network that scores each expert\n",
    "2. **Top-K Selection**: Choose K experts with highest scores\n",
    "3. **Softmax Normalization**: Convert scores to weights\n",
    "4. **Weighted Combination**: Combine expert outputs\n",
    "\n",
    "$$\\text{Router}(x) = \\text{Top-K}(\\text{Softmax}(W_r \\cdot x))$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TopKRouter(nn.Module):\n",
    "    \"\"\"\n",
    "    Top-K router for Mixture of Experts.\n",
    "\n",
    "    Routes each token to the top-K experts based on learned routing scores.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        hidden_size: int,\n",
    "        num_experts: int,\n",
    "        top_k: int = 2,\n",
    "        use_load_balancing: bool = True,\n",
    "        load_balancing_weight: float = 0.01,\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        self.num_experts = num_experts\n",
    "        self.top_k = top_k\n",
    "        self.use_load_balancing = use_load_balancing\n",
    "        self.load_balancing_weight = load_balancing_weight\n",
    "\n",
    "        # Router is a simple linear layer\n",
    "        self.gate = nn.Linear(hidden_size, num_experts, bias=False)\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> Tuple[torch.Tensor, torch.Tensor, Dict]:\n",
    "        \"\"\"\n",
    "        Route tokens to top-K experts.\n",
    "\n",
    "        Args:\n",
    "            x: Input tensor (batch_size, seq_len, hidden_size)\n",
    "\n",
    "        Returns:\n",
    "            expert_indices: Indices of selected experts (batch, seq_len, top_k)\n",
    "            expert_weights: Weights for selected experts (batch, seq_len, top_k)\n",
    "            stats: Dictionary of routing statistics\n",
    "        \"\"\"\n",
    "        batch_size, seq_len, hidden_size = x.shape\n",
    "\n",
    "        # Flatten batch and sequence dimensions\n",
    "        x_flat = x.view(-1, hidden_size)  # (batch * seq_len, hidden_size)\n",
    "\n",
    "        # Compute routing logits\n",
    "        logits = self.gate(x_flat)  # (batch * seq_len, num_experts)\n",
    "\n",
    "        # Apply softmax to get probabilities\n",
    "        routing_probs = F.softmax(logits, dim=-1)\n",
    "\n",
    "        # Select top-K experts\n",
    "        expert_weights, expert_indices = torch.topk(routing_probs, self.top_k, dim=-1)\n",
    "\n",
    "        # Normalize weights to sum to 1\n",
    "        expert_weights = expert_weights / expert_weights.sum(dim=-1, keepdim=True)\n",
    "\n",
    "        # Reshape back\n",
    "        expert_indices = expert_indices.view(batch_size, seq_len, self.top_k)\n",
    "        expert_weights = expert_weights.view(batch_size, seq_len, self.top_k)\n",
    "\n",
    "        # Compute statistics for monitoring\n",
    "        stats = self._compute_stats(routing_probs, expert_indices)\n",
    "\n",
    "        # Compute load balancing loss if enabled\n",
    "        if self.use_load_balancing and self.training:\n",
    "            stats[\"load_balancing_loss\"] = self._load_balancing_loss(routing_probs)\n",
    "\n",
    "        return expert_indices, expert_weights, stats\n",
    "\n",
    "    def _compute_stats(\n",
    "        self, routing_probs: torch.Tensor, expert_indices: torch.Tensor\n",
    "    ) -> Dict:\n",
    "        \"\"\"Compute routing statistics for monitoring.\"\"\"\n",
    "        stats = {}\n",
    "\n",
    "        # Expert utilization (how many tokens are routed to each expert)\n",
    "        expert_counts = torch.zeros(self.num_experts, device=routing_probs.device)\n",
    "        for i in range(self.num_experts):\n",
    "            expert_counts[i] = (expert_indices == i).sum()\n",
    "\n",
    "        # Normalize to get distribution\n",
    "        expert_distribution = expert_counts / expert_counts.sum()\n",
    "        stats[\"expert_distribution\"] = expert_distribution\n",
    "\n",
    "        # Balance metric (how evenly distributed)\n",
    "        # Perfect balance = 1.0, completely unbalanced = 0.0\n",
    "        uniform_dist = torch.ones_like(expert_distribution) / self.num_experts\n",
    "        balance_metric = (\n",
    "            1.0 - torch.sum(torch.abs(expert_distribution - uniform_dist)) / 2.0\n",
    "        )\n",
    "        stats[\"expert_balance_metric\"] = balance_metric.item()\n",
    "\n",
    "        # Routing entropy (higher = more diverse routing)\n",
    "        entropy = -torch.sum(\n",
    "            routing_probs * torch.log(routing_probs + 1e-10), dim=-1\n",
    "        ).mean()\n",
    "        stats[\"routing_entropy\"] = entropy.item()\n",
    "\n",
    "        return stats\n",
    "\n",
    "    def _load_balancing_loss(self, routing_probs: torch.Tensor) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        Compute load balancing loss to encourage even expert utilization.\n",
    "\n",
    "        This is the auxiliary loss from the Switch Transformer paper.\n",
    "        \"\"\"\n",
    "        # Average routing probability per expert\n",
    "        expert_probs = routing_probs.mean(dim=0)  # (num_experts,)\n",
    "\n",
    "        # Count how many tokens are routed to each expert\n",
    "        top1_indices = routing_probs.argmax(dim=-1)\n",
    "        expert_counts = torch.zeros_like(expert_probs)\n",
    "        for i in range(self.num_experts):\n",
    "            expert_counts[i] = (top1_indices == i).float().mean()\n",
    "\n",
    "        # Load balancing loss\n",
    "        loss = self.num_experts * torch.sum(expert_probs * expert_counts)\n",
    "        return self.load_balancing_weight * loss\n",
    "\n",
    "\n",
    "# Test the router\n",
    "hidden_size = 512\n",
    "num_experts = 8\n",
    "top_k = 2\n",
    "batch_size = 4\n",
    "seq_len = 10\n",
    "\n",
    "router = TopKRouter(hidden_size, num_experts, top_k)\n",
    "x = torch.randn(batch_size, seq_len, hidden_size)\n",
    "\n",
    "expert_indices, expert_weights, stats = router(x)\n",
    "\n",
    "print(f\"Input shape: {x.shape}\")\n",
    "print(f\"Expert indices shape: {expert_indices.shape}\")\n",
    "print(f\"Expert weights shape: {expert_weights.shape}\")\n",
    "print(\"\\nRouting Statistics:\")\n",
    "print(f\"  Balance metric: {stats['expert_balance_metric']:.3f} (1.0 = perfect balance)\")\n",
    "print(f\"  Routing entropy: {stats['routing_entropy']:.3f}\")\n",
    "print(\"\\nExample routing for first token:\")\n",
    "print(f\"  Selected experts: {expert_indices[0, 0].tolist()}\")\n",
    "print(f\"  Expert weights: {expert_weights[0, 0].tolist()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualizing Expert Routing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualize_routing(expert_indices, expert_weights, num_experts=8):\n",
    "    \"\"\"\n",
    "    Visualize which experts are chosen for different tokens.\n",
    "    \"\"\"\n",
    "    batch_size, seq_len, top_k = expert_indices.shape\n",
    "\n",
    "    # Take first batch\n",
    "    indices = expert_indices[0].cpu().numpy()\n",
    "    weights = expert_weights[0].cpu().numpy()\n",
    "\n",
    "    # Create routing matrix (seq_len x num_experts)\n",
    "    routing_matrix = np.zeros((seq_len, num_experts))\n",
    "    for i in range(seq_len):\n",
    "        for k in range(top_k):\n",
    "            expert_idx = indices[i, k]\n",
    "            weight = weights[i, k]\n",
    "            routing_matrix[i, expert_idx] = weight\n",
    "\n",
    "    # Plot\n",
    "    plt.figure(figsize=(12, 6))\n",
    "    sns.heatmap(\n",
    "        routing_matrix,\n",
    "        annot=True,\n",
    "        fmt=\".2f\",\n",
    "        cmap=\"YlOrRd\",\n",
    "        xticklabels=[f\"E{i}\" for i in range(num_experts)],\n",
    "        yticklabels=[f\"T{i}\" for i in range(seq_len)],\n",
    "        cbar_kws={\"label\": \"Routing Weight\"},\n",
    "    )\n",
    "    plt.xlabel(\"Expert\")\n",
    "    plt.ylabel(\"Token Position\")\n",
    "    plt.title(\"Token-to-Expert Routing Pattern\")\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "visualize_routing(expert_indices, expert_weights, num_experts)\n",
    "print(\"Each row shows which experts process that token (brighter = higher weight)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 3: Building the Expert Layer\n",
    "\n",
    "Each expert is typically a feed-forward network (FFN), identical in architecture but with different learned parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Expert(nn.Module):\n",
    "    \"\"\"\n",
    "    Single expert: a feed-forward network.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, hidden_size: int, intermediate_size: int, dropout: float = 0.1):\n",
    "        super().__init__()\n",
    "        self.fc1 = nn.Linear(hidden_size, intermediate_size)\n",
    "        self.fc2 = nn.Linear(intermediate_size, hidden_size)\n",
    "        self.activation = nn.GELU()\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        \"\"\"Forward pass through expert.\"\"\"\n",
    "        x = self.fc1(x)\n",
    "        x = self.activation(x)\n",
    "        x = self.dropout(x)\n",
    "        x = self.fc2(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "# Test a single expert\n",
    "expert = Expert(hidden_size=512, intermediate_size=2048)\n",
    "x = torch.randn(4, 10, 512)\n",
    "output = expert(x)\n",
    "print(f\"Expert input: {x.shape}\")\n",
    "print(f\"Expert output: {output.shape}\")\n",
    "print(f\"Expert parameters: {sum(p.numel() for p in expert.parameters()):,}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 4: Complete MoE Layer\n",
    "\n",
    "Now let's combine the router and experts into a complete MoE layer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MoELayer(nn.Module):\n",
    "    \"\"\"\n",
    "    Mixture of Experts layer with Top-K routing.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        hidden_size: int,\n",
    "        intermediate_size: int,\n",
    "        num_experts: int = 8,\n",
    "        top_k: int = 2,\n",
    "        dropout: float = 0.1,\n",
    "        use_load_balancing: bool = True,\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        self.num_experts = num_experts\n",
    "        self.top_k = top_k\n",
    "\n",
    "        # Create experts\n",
    "        self.experts = nn.ModuleList(\n",
    "            [\n",
    "                Expert(hidden_size, intermediate_size, dropout)\n",
    "                for _ in range(num_experts)\n",
    "            ]\n",
    "        )\n",
    "\n",
    "        # Create router\n",
    "        self.router = TopKRouter(hidden_size, num_experts, top_k, use_load_balancing)\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> Tuple[torch.Tensor, Dict]:\n",
    "        \"\"\"\n",
    "        Forward pass through MoE layer.\n",
    "\n",
    "        Args:\n",
    "            x: Input tensor (batch_size, seq_len, hidden_size)\n",
    "\n",
    "        Returns:\n",
    "            output: MoE output (batch_size, seq_len, hidden_size)\n",
    "            stats: Routing statistics\n",
    "        \"\"\"\n",
    "        batch_size, seq_len, hidden_size = x.shape\n",
    "\n",
    "        # Get routing decisions\n",
    "        expert_indices, expert_weights, stats = self.router(x)\n",
    "\n",
    "        # Initialize output\n",
    "        output = torch.zeros_like(x)\n",
    "\n",
    "        # Process each expert\n",
    "        # Note: This is a naive implementation. Production systems use\n",
    "        # more efficient batching strategies.\n",
    "        for expert_idx in range(self.num_experts):\n",
    "            # Find all tokens routed to this expert\n",
    "            expert_mask = (expert_indices == expert_idx).any(dim=-1)  # (batch, seq_len)\n",
    "\n",
    "            if not expert_mask.any():\n",
    "                continue  # No tokens for this expert\n",
    "\n",
    "            # Get tokens for this expert\n",
    "            expert_input = x[expert_mask]  # (num_tokens, hidden_size)\n",
    "\n",
    "            # Process through expert\n",
    "            expert_output = self.experts[expert_idx](expert_input)\n",
    "\n",
    "            # Get weights for this expert\n",
    "            # This is a bit tricky - we need to find which positions in top_k\n",
    "            # correspond to this expert\n",
    "            for k in range(self.top_k):\n",
    "                k_mask = expert_indices[:, :, k] == expert_idx\n",
    "                if k_mask.any():\n",
    "                    weights = expert_weights[:, :, k][k_mask].unsqueeze(-1)\n",
    "                    output[k_mask] += weights * expert_output[: weights.shape[0]]\n",
    "\n",
    "        return output, stats\n",
    "\n",
    "\n",
    "# Test MoE layer\n",
    "moe_layer = MoELayer(hidden_size=512, intermediate_size=2048, num_experts=8, top_k=2)\n",
    "\n",
    "x = torch.randn(4, 10, 512)\n",
    "output, stats = moe_layer(x)\n",
    "\n",
    "print(f\"Input shape: {x.shape}\")\n",
    "print(f\"Output shape: {output.shape}\")\n",
    "print(\"\\nMoE Layer Statistics:\")\n",
    "print(f\"  Expert balance: {stats['expert_balance_metric']:.3f}\")\n",
    "print(f\"  Routing entropy: {stats['routing_entropy']:.3f}\")\n",
    "print(f\"\\nTotal parameters: {sum(p.numel() for p in moe_layer.parameters()):,}\")\n",
    "print(\n",
    "    f\"Active parameters per token: ~{sum(p.numel() for p in moe_layer.experts[0].parameters()) * 2:,}\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 5: Load Balancing - A Critical Challenge\n",
    "\n",
    "### The Problem\n",
    "\n",
    "Without proper incentives, the model might use only a few experts and ignore others!\n",
    "\n",
    "### Solutions:\n",
    "\n",
    "1. **Load Balancing Loss**: Penalize uneven expert usage\n",
    "2. **Capacity Factor**: Limit tokens per expert\n",
    "3. **Expert Dropout**: Randomly drop experts during training\n",
    "\n",
    "Let's visualize the effect of load balancing:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train two routers: one with and one without load balancing\n",
    "torch.manual_seed(42)\n",
    "router_with_lb = TopKRouter(512, 8, 2, use_load_balancing=True)\n",
    "router_without_lb = TopKRouter(512, 8, 2, use_load_balancing=False)\n",
    "\n",
    "# Simulate some training steps\n",
    "x = torch.randn(32, 20, 512)  # Larger batch\n",
    "\n",
    "# Get routing decisions\n",
    "indices_with, weights_with, stats_with = router_with_lb(x)\n",
    "indices_without, weights_without, stats_without = router_without_lb(x)\n",
    "\n",
    "# Plot expert utilization\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "# With load balancing\n",
    "dist_with = stats_with[\"expert_distribution\"].cpu().numpy()\n",
    "ax1.bar(range(8), dist_with, color=\"steelblue\", alpha=0.8)\n",
    "ax1.axhline(y=1 / 8, color=\"red\", linestyle=\"--\", label=\"Ideal (uniform)\")\n",
    "ax1.set_xlabel(\"Expert ID\")\n",
    "ax1.set_ylabel(\"Utilization\")\n",
    "ax1.set_title(\n",
    "    f\"With Load Balancing (Balance: {stats_with['expert_balance_metric']:.3f})\"\n",
    ")\n",
    "ax1.legend()\n",
    "ax1.set_ylim(0, 0.3)\n",
    "\n",
    "# Without load balancing\n",
    "dist_without = stats_without[\"expert_distribution\"].cpu().numpy()\n",
    "ax2.bar(range(8), dist_without, color=\"coral\", alpha=0.8)\n",
    "ax2.axhline(y=1 / 8, color=\"red\", linestyle=\"--\", label=\"Ideal (uniform)\")\n",
    "ax2.set_xlabel(\"Expert ID\")\n",
    "ax2.set_ylabel(\"Utilization\")\n",
    "ax2.set_title(\n",
    "    f\"Without Load Balancing (Balance: {stats_without['expert_balance_metric']:.3f})\"\n",
    ")\n",
    "ax2.legend()\n",
    "ax2.set_ylim(0, 0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"Notice: Load balancing encourages more uniform expert usage!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 6: MoE Transformer Block\n",
    "\n",
    "Let's integrate MoE into a complete transformer block. We typically apply MoE to the FFN layer every N layers (controlled by `moe_frequency`)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import attention from previous notebook\n",
    "class MultiHeadAttention(nn.Module):\n",
    "    \"\"\"Simplified version - see notebook 02 for full implementation.\"\"\"\n",
    "\n",
    "    def __init__(self, d_model, num_heads, dropout=0.1):\n",
    "        super().__init__()\n",
    "        self.d_model = d_model\n",
    "        self.num_heads = num_heads\n",
    "        self.d_k = d_model // num_heads\n",
    "\n",
    "        self.W_q = nn.Linear(d_model, d_model)\n",
    "        self.W_k = nn.Linear(d_model, d_model)\n",
    "        self.W_v = nn.Linear(d_model, d_model)\n",
    "        self.W_o = nn.Linear(d_model, d_model)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, x, mask=None):\n",
    "        batch_size, seq_len, _ = x.size()\n",
    "\n",
    "        # Simplified: just return projection for demonstration\n",
    "        # See notebook 02 for complete implementation\n",
    "        output = self.W_o(x)\n",
    "        return output\n",
    "\n",
    "\n",
    "class MoETransformerBlock(nn.Module):\n",
    "    \"\"\"\n",
    "    Transformer block with MoE in the FFN layer.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        d_model: int,\n",
    "        num_heads: int,\n",
    "        d_ff: int,\n",
    "        num_experts: int = 8,\n",
    "        top_k: int = 2,\n",
    "        dropout: float = 0.1,\n",
    "        use_moe: bool = True,\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.use_moe = use_moe\n",
    "\n",
    "        # Multi-head attention\n",
    "        self.attention = MultiHeadAttention(d_model, num_heads, dropout)\n",
    "\n",
    "        # Feed-forward: either MoE or dense\n",
    "        if use_moe:\n",
    "            self.ffn = MoELayer(\n",
    "                hidden_size=d_model,\n",
    "                intermediate_size=d_ff,\n",
    "                num_experts=num_experts,\n",
    "                top_k=top_k,\n",
    "                dropout=dropout,\n",
    "            )\n",
    "        else:\n",
    "            self.ffn = Expert(d_model, d_ff, dropout)\n",
    "\n",
    "        # Layer norms\n",
    "        self.ln1 = nn.LayerNorm(d_model)\n",
    "        self.ln2 = nn.LayerNorm(d_model)\n",
    "\n",
    "        # Dropout\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, x, mask=None):\n",
    "        \"\"\"\n",
    "        Forward pass through MoE transformer block.\n",
    "\n",
    "        Returns:\n",
    "            output: Block output\n",
    "            moe_stats: MoE routing statistics (if MoE is used)\n",
    "        \"\"\"\n",
    "        # 1. Attention with residual\n",
    "        attn_output = self.attention(x, mask)\n",
    "        x = self.ln1(x + self.dropout(attn_output))\n",
    "\n",
    "        # 2. FFN (MoE or dense) with residual\n",
    "        if self.use_moe:\n",
    "            ffn_output, moe_stats = self.ffn(x)\n",
    "        else:\n",
    "            ffn_output = self.ffn(x)\n",
    "            moe_stats = None\n",
    "\n",
    "        x = self.ln2(x + self.dropout(ffn_output))\n",
    "\n",
    "        return x, moe_stats\n",
    "\n",
    "\n",
    "# Compare MoE vs Dense block\n",
    "moe_block = MoETransformerBlock(d_model=512, num_heads=8, d_ff=2048, use_moe=True)\n",
    "dense_block = MoETransformerBlock(d_model=512, num_heads=8, d_ff=2048, use_moe=False)\n",
    "\n",
    "x = torch.randn(4, 10, 512)\n",
    "moe_out, stats = moe_block(x)\n",
    "dense_out, _ = dense_block(x)\n",
    "\n",
    "moe_params = sum(p.numel() for p in moe_block.parameters())\n",
    "dense_params = sum(p.numel() for p in dense_block.parameters())\n",
    "\n",
    "print(f\"MoE Block Parameters: {moe_params:,}\")\n",
    "print(f\"Dense Block Parameters: {dense_params:,}\")\n",
    "print(f\"\\nParameter Ratio: {moe_params / dense_params:.2f}x more parameters\")\n",
    "print(f\"Compute Ratio: ~{2 / 8:.2f}x (using 2 of 8 experts)\")\n",
    "print(\n",
    "    f\"\\nEfficiency Gain: {(moe_params / dense_params) / (2 / 8):.2f}x more parameters per unit compute!\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 7: Understanding the Efficiency Gains"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def analyze_model_efficiency(d_model, d_ff, num_experts, top_k, num_layers):\n",
    "    \"\"\"\n",
    "    Analyze parameter count and compute cost for MoE vs Dense.\n",
    "    \"\"\"\n",
    "    # Dense FFN parameters per layer\n",
    "    dense_ffn_params = d_model * d_ff + d_ff + d_ff * d_model + d_model\n",
    "\n",
    "    # MoE FFN parameters per layer\n",
    "    moe_ffn_params = num_experts * dense_ffn_params + d_model * num_experts  # +router\n",
    "\n",
    "    # Attention parameters (same for both)\n",
    "    attn_params = 4 * d_model * d_model  # Q, K, V, O projections\n",
    "\n",
    "    # Total parameters\n",
    "    dense_total = num_layers * (attn_params + dense_ffn_params)\n",
    "    moe_total = num_layers * (attn_params + moe_ffn_params)\n",
    "\n",
    "    # Active parameters per forward pass\n",
    "    dense_active = dense_total\n",
    "    moe_active = num_layers * (attn_params + top_k * dense_ffn_params / num_experts)\n",
    "\n",
    "    return {\n",
    "        \"dense_total\": dense_total,\n",
    "        \"moe_total\": moe_total,\n",
    "        \"dense_active\": dense_active,\n",
    "        \"moe_active\": moe_active,\n",
    "    }\n",
    "\n",
    "\n",
    "# Our Storyteller model configuration\n",
    "results = analyze_model_efficiency(\n",
    "    d_model=1024, d_ff=4096, num_experts=8, top_k=2, num_layers=16\n",
    ")\n",
    "\n",
    "print(\"Storyteller Model Analysis:\")\n",
    "print(\"=\" * 50)\n",
    "print(\"\\nIf we used Dense FFN everywhere:\")\n",
    "print(f\"  Total parameters: {results['dense_total'] / 1e6:.1f}M\")\n",
    "print(f\"  Active per forward: {results['dense_active'] / 1e6:.1f}M\")\n",
    "print(\"\\nWith MoE (8 experts, top-2):\")\n",
    "print(f\"  Total parameters: {results['moe_total'] / 1e6:.1f}M\")\n",
    "print(f\"  Active per forward: {results['moe_active'] / 1e6:.1f}M\")\n",
    "print(\"\\nKey Insights:\")\n",
    "print(f\"  Parameter increase: {results['moe_total'] / results['dense_total']:.2f}x\")\n",
    "print(f\"  Compute increase: {results['moe_active'] / results['dense_active']:.2f}x\")\n",
    "print(\n",
    "    f\"  Efficiency ratio: {(results['moe_total'] / results['dense_total']) / (results['moe_active'] / results['dense_active']):.2f}x\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 8: Practical Considerations\n",
    "\n",
    "### When to Use MoE?\n",
    "\n",
    "**Good for:**\n",
    "- Large-scale models (billions of parameters)\n",
    "- Diverse tasks (different experts for different patterns)\n",
    "- Training efficiency (more parameters without proportional compute)\n",
    "\n",
    "**Challenges:**\n",
    "- Load balancing requires careful tuning\n",
    "- Communication overhead in distributed training\n",
    "- Larger memory footprint (storing all experts)\n",
    "- More complex implementation\n",
    "\n",
    "### Hyperparameters to Tune:\n",
    "\n",
    "1. **Number of experts**: 8-64 typical (power of 2)\n",
    "2. **Top-K**: Usually 1 or 2\n",
    "3. **MoE frequency**: Every 2-4 layers\n",
    "4. **Load balancing weight**: 0.01-0.1\n",
    "5. **Capacity factor**: 1.0-2.0 (for capacity-based routing)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary and Key Takeaways\n",
    "\n",
    "In this notebook, you learned:\n",
    "\n",
    "1. **MoE Motivation**:\n",
    "   - Scale model capacity without proportional compute cost\n",
    "   - Conditional computation via expert routing\n",
    "\n",
    "2. **Core Components**:\n",
    "   - **Router**: Learns which experts to use for each token\n",
    "   - **Experts**: Specialized feed-forward networks\n",
    "   - **Top-K Gating**: Select K best experts per token\n",
    "\n",
    "3. **Load Balancing**:\n",
    "   - Critical for utilizing all experts\n",
    "   - Auxiliary loss encourages even distribution\n",
    "\n",
    "4. **Efficiency**:\n",
    "   - 4-8x more parameters, only 1.5-2x more compute\n",
    "   - Sub-linear scaling of compute with capacity\n",
    "\n",
    "5. **Implementation**:\n",
    "   - Built complete MoE layer from scratch\n",
    "   - Integrated into transformer blocks\n",
    "   - Monitored routing statistics\n",
    "\n",
    "### What's Next?\n",
    "\n",
    "In the next notebook, we'll:\n",
    "- Set up the complete training pipeline\n",
    "- Implement data loading and batching\n",
    "- Configure optimizers and learning rate schedules\n",
    "- Train a small MoE model\n",
    "\n",
    "### Further Reading\n",
    "\n",
    "- [Outrageously Large Neural Networks: The Sparsely-Gated MoE Layer](https://arxiv.org/abs/1701.06538) (Shazeer et al., 2017)\n",
    "- [Switch Transformers](https://arxiv.org/abs/2101.03961) (Fedus et al., 2021)\n",
    "- [GLaM: Efficient Scaling of Language Models](https://arxiv.org/abs/2112.06905) (Du et al., 2021)\n",
    "- [ST-MoE: Designing Stable and Transferable MoE](https://arxiv.org/abs/2202.08906) (Zoph et al., 2022)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise: Design Your Own MoE Configuration\n",
    "\n",
    "Given these constraints:\n",
    "- 32GB GPU memory\n",
    "- Target: ~500M total parameters\n",
    "- Want to maximize capacity while keeping training feasible\n",
    "\n",
    "Design an MoE configuration and justify your choices:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your configuration here\n",
    "config = {\n",
    "    \"d_model\": 1024,  # Hidden size\n",
    "    \"d_ff\": 4096,  # FFN intermediate size\n",
    "    \"num_layers\": 16,  # Number of transformer layers\n",
    "    \"num_experts\": 8,  # Experts per MoE layer\n",
    "    \"top_k\": 2,  # Experts activated per token\n",
    "    \"moe_frequency\": 2,  # Apply MoE every N layers\n",
    "}\n",
    "\n",
    "# Analyze your configuration\n",
    "# TODO: Estimate total parameters, memory usage, and training throughput\n",
    "\n",
    "# Justify your choices:\n",
    "# - Why this number of experts?\n",
    "# - Why this top-k value?\n",
    "# - Why this MoE frequency?\n",
    "# - What tradeoffs did you make?"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
