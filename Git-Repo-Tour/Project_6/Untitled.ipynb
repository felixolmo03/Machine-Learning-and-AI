{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b6e8bc8-fa0e-474d-8b4d-12e439e15f4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pygame\n",
    "import random\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from collections import deque\n",
    "import pickle\n",
    "import os\n",
    "from datetime import datetime\n",
    "\n",
    "# Initialize Pygame and Constants\n",
    "\n",
    "pygame.init()\n",
    "\n",
    "# Colors (RGB)\n",
    "\n",
    "BLACK = (0, 0, 0)\n",
    "WHITE = (255, 255, 255)\n",
    "CYAN = (0, 255, 255)\n",
    "BLUE = (0, 0, 255)\n",
    "ORANGE = (255, 165, 0)\n",
    "YELLOW = (255, 255, 0)\n",
    "GREEN = (0, 255, 0)\n",
    "PURPLE = (128, 0, 128)\n",
    "RED = (255, 0, 0)\n",
    "GRAY = (128, 128, 128)\n",
    "DARK_GRAY = (50, 50, 50)\n",
    "\n",
    "# Game dimensions\n",
    "\n",
    "BLOCK_SIZE = 30\n",
    "GRID_WIDTH = 10\n",
    "GRID_HEIGHT = 20\n",
    "SCREEN_WIDTH = BLOCK_SIZE * (GRID_WIDTH + 8)\n",
    "SCREEN_HEIGHT = BLOCK_SIZE * GRID_HEIGHT\n",
    "\n",
    "# Tetromino shapes\n",
    "\n",
    "SHAPES = [\n",
    "    np.array([[1, 1, 1, 1]], dtype=int),  # I\n",
    "    np.array([[1, 1, 1], [0, 1, 0]], dtype=int),  # T\n",
    "    np.array([[1, 1, 1], [1, 0, 0]], dtype=int),  # L\n",
    "    np.array([[1, 1, 1], [0, 0, 1]], dtype=int),  # J\n",
    "    np.array([[1, 1], [1, 1]], dtype=int),  # O\n",
    "    np.array([[0, 1, 1], [1, 1, 0]], dtype=int),  # S\n",
    "    np.array([[1, 1, 0], [0, 1, 1]], dtype=int)  # Z\n",
    "]\n",
    "SHAPE_COLORS = [CYAN, PURPLE, ORANGE, BLUE, YELLOW, GREEN, RED]\n",
    "SHAPE_NAMES = [\"I\", \"T\", \"L\", \"J\", \"O\", \"S\", \"Z\"]\n",
    "\n",
    "# Define actions\n",
    "\n",
    "ACTION_MOVE_LEFT = 0\n",
    "ACTION_MOVE_RIGHT = 1\n",
    "ACTION_ROTATE = 2\n",
    "ACTION_HARD_DROP = 3\n",
    "\n",
    "ACTION_NAMES = {\n",
    "    ACTION_MOVE_LEFT: \"Left\",\n",
    "    ACTION_MOVE_RIGHT: \"Right\",\n",
    "    ACTION_ROTATE: \"Rotate\",\n",
    "    ACTION_HARD_DROP: \"Hard Drop\"\n",
    "}\n",
    "\n",
    "NUM_ACTIONS = 4\n",
    "\n",
    "class DQNetwork(nn.Module):\n",
    "    \"\"\"Deep Q-Network for Tetris\"\"\"\n",
    "    def __init__(self, input_size, output_size):\n",
    "        super(DQNetwork, self).__init__()\n",
    "        self.fc1 = nn.Linear(input_size, 128)\n",
    "        self.fc2 = nn.Linear(128, 128)\n",
    "        self.fc3 = nn.Linear(128, 64)\n",
    "        self.fc4 = nn.Linear(64, output_size)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = torch.relu(self.fc1(x))\n",
    "        x = torch.relu(self.fc2(x))\n",
    "        x = torch.relu(self.fc3(x))\n",
    "        return self.fc4(x)\n",
    "\n",
    "class ReplayBuffer:\n",
    "    \"\"\"Experience replay buffer for DQN\"\"\"\n",
    "    def __init__(self, capacity=10000):\n",
    "        self.buffer = deque(maxlen=capacity)\n",
    "\n",
    "    def push(self, state, action, reward, next_state, done):\n",
    "        self.buffer.append((state, action, reward, next_state, done))\n",
    "\n",
    "    def sample(self, batch_size):\n",
    "        batch = random.sample(self.buffer, batch_size)\n",
    "        states, actions, rewards, next_states, dones = zip(*batch)\n",
    "        return (np.array(states), np.array(actions), np.array(rewards), \n",
    "                np.array(next_states), np.array(dones))\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.buffer)\n",
    "\n",
    "class DQNAgent:\n",
    "    \"\"\"DQN Agent for Tetris\"\"\"\n",
    "    def __init__(self, state_size, action_size, device='cpu'):\n",
    "        self.state_size = state_size\n",
    "        self.action_size = action_size\n",
    "        self.device = device\n",
    "\n",
    "        # Hyperparameters\n",
    "        self.gamma = 0.99  # Discount factor\n",
    "        self.epsilon = 1.0  # Exploration rate\n",
    "        self.epsilon_min = 0.01\n",
    "        self.epsilon_decay = 0.9995\n",
    "        self.learning_rate = 0.001\n",
    "        self.batch_size = 64\n",
    "        self.update_target_every = 1000\n",
    "        \n",
    "        # Networks\n",
    "        self.policy_net = DQNetwork(state_size, action_size).to(device)\n",
    "        self.target_net = DQNetwork(state_size, action_size).to(device)\n",
    "        self.target_net.load_state_dict(self.policy_net.state_dict())\n",
    "        self.target_net.eval()\n",
    "        \n",
    "        self.optimizer = optim.Adam(self.policy_net.parameters(), lr=self.learning_rate)\n",
    "        self.memory = ReplayBuffer(capacity=10000)\n",
    "        self.steps = 0\n",
    "        \n",
    "    def get_state_features(self, game):\n",
    "        \"\"\"Extract features from game state\"\"\"\n",
    "        grid = game.grid.copy()\n",
    "        \n",
    "        # Column heights\n",
    "        heights = []\n",
    "        for col in range(GRID_WIDTH):\n",
    "            height = 0\n",
    "            for row in range(GRID_HEIGHT):\n",
    "                if grid[row, col] != 0:\n",
    "                    height = GRID_HEIGHT - row\n",
    "                    break\n",
    "            heights.append(height)\n",
    "        \n",
    "        # Holes (empty cells with filled cells above)\n",
    "        holes = 0\n",
    "        for col in range(GRID_WIDTH):\n",
    "            block_found = False\n",
    "            for row in range(GRID_HEIGHT):\n",
    "                if grid[row, col] != 0:\n",
    "                    block_found = True\n",
    "                elif block_found and grid[row, col] == 0:\n",
    "                    holes += 1\n",
    "        \n",
    "        # Bumpiness (height differences between adjacent columns)\n",
    "        bumpiness = sum(abs(heights[i] - heights[i+1]) for i in range(len(heights)-1))\n",
    "        \n",
    "        # Total height\n",
    "        total_height = sum(heights)\n",
    "        \n",
    "        # Max height\n",
    "        max_height = max(heights) if heights else 0\n",
    "        \n",
    "        # Complete lines\n",
    "        complete_lines = sum(1 for row in grid if all(row))\n",
    "        \n",
    "        # Create feature vector\n",
    "        features = heights + [holes, bumpiness, total_height, max_height, complete_lines]\n",
    "        return np.array(features, dtype=np.float32)\n",
    "\n",
    "    def select_action(self, state, training=True):\n",
    "        \"\"\"Select action using epsilon-greedy policy\"\"\"\n",
    "        if training and random.random() < self.epsilon:\n",
    "            return random.randint(0, self.action_size - 1)\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            state_tensor = torch.FloatTensor(state).unsqueeze(0).to(self.device)\n",
    "            q_values = self.policy_net(state_tensor)\n",
    "            return q_values.argmax().item()\n",
    "\n",
    "    def train_step(self):\n",
    "        \"\"\"Perform one training step\"\"\"\n",
    "        if len(self.memory) < self.batch_size:\n",
    "            return None\n",
    "        \n",
    "        # Sample batch\n",
    "        states, actions, rewards, next_states, dones = self.memory.sample(self.batch_size)\n",
    "        \n",
    "        # Convert to tensors\n",
    "        states = torch.FloatTensor(states).to(self.device)\n",
    "        actions = torch.LongTensor(actions).to(self.device)\n",
    "        rewards = torch.FloatTensor(rewards).to(self.device)\n",
    "        next_states = torch.FloatTensor(next_states).to(self.device)\n",
    "        dones = torch.FloatTensor(dones).to(self.device)\n",
    "        \n",
    "        # Current Q values\n",
    "        current_q_values = self.policy_net(states).gather(1, actions.unsqueeze(1)).squeeze(1)\n",
    "        \n",
    "        # Next Q values\n",
    "        with torch.no_grad():\n",
    "            next_q_values = self.target_net(next_states).max(1)[0]\n",
    "            target_q_values = rewards + (1 - dones) * self.gamma * next_q_values\n",
    "        \n",
    "        # Compute loss\n",
    "        loss = nn.MSELoss()(current_q_values, target_q_values)\n",
    "        \n",
    "        # Optimize\n",
    "        self.optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        self.optimizer.step()\n",
    "        \n",
    "        # Update target network\n",
    "        self.steps += 1\n",
    "        if self.steps % self.update_target_every == 0:\n",
    "            self.target_net.load_state_dict(self.policy_net.state_dict())\n",
    "        \n",
    "        # Decay epsilon\n",
    "        if self.epsilon > self.epsilon_min:\n",
    "            self.epsilon *= self.epsilon_decay\n",
    "        \n",
    "        return loss.item()\n",
    "\n",
    "    def save(self, filepath):\n",
    "        \"\"\"Save model\"\"\"\n",
    "        torch.save({\n",
    "            'policy_net': self.policy_net.state_dict(),\n",
    "            'target_net': self.target_net.state_dict(),\n",
    "            'optimizer': self.optimizer.state_dict(),\n",
    "            'epsilon': self.epsilon,\n",
    "            'steps': self.steps\n",
    "        }, filepath)\n",
    "\n",
    "    def load(self, filepath):\n",
    "        \"\"\"Load model\"\"\"\n",
    "        checkpoint = torch.load(filepath, map_location=self.device)\n",
    "        self.policy_net.load_state_dict(checkpoint['policy_net'])\n",
    "        self.target_net.load_state_dict(checkpoint['target_net'])\n",
    "        self.optimizer.load_state_dict(checkpoint['optimizer'])\n",
    "        self.epsilon = checkpoint['epsilon']\n",
    "        self.steps = checkpoint['steps']\n",
    "\n",
    "class Tetris:\n",
    "    def __init__(self):\n",
    "        self.grid = np.zeros((GRID_HEIGHT, GRID_WIDTH), dtype=int)\n",
    "        self.current_piece = self.new_piece()\n",
    "        self.next_piece = self.new_piece()\n",
    "        self.game_over = False\n",
    "        self.score = 0\n",
    "        self.lines_cleared = 0\n",
    "        self.fall_time = 0\n",
    "        self.fall_speed = 500\n",
    "        self.last_action = None\n",
    "        self.pieces_placed = 0\n",
    "\n",
    "    def new_piece(self):\n",
    "        shape_idx = random.randint(0, len(SHAPES) - 1)\n",
    "        return {\n",
    "            'shape': SHAPES[shape_idx].copy(),\n",
    "            'color': SHAPE_COLORS[shape_idx],\n",
    "            'name': SHAPE_NAMES[shape_idx],\n",
    "            'x': GRID_WIDTH // 2 - SHAPES[shape_idx].shape[1] // 2,\n",
    "            'y': 0\n",
    "        }\n",
    "\n",
    "    def valid_move(self, piece, x_offset=0, y_offset=0, rotated_shape=None):\n",
    "        shape_to_check = piece['shape'] if rotated_shape is None else rotated_shape\n",
    "        for y, row in enumerate(shape_to_check):\n",
    "            for x, cell in enumerate(row):\n",
    "                if cell:\n",
    "                    new_x = piece['x'] + x + x_offset\n",
    "                    new_y = piece['y'] + y + y_offset\n",
    "                    if (new_x < 0 or new_x >= GRID_WIDTH or\n",
    "                            new_y >= GRID_HEIGHT or\n",
    "                            (new_y >= 0 and self.grid[new_y, new_x])):\n",
    "                        return False\n",
    "        return True\n",
    "\n",
    "    def rotate_piece(self):\n",
    "        rotated_shape = np.rot90(self.current_piece['shape'], 3)\n",
    "        if self.valid_move(self.current_piece, rotated_shape=rotated_shape):\n",
    "            self.current_piece['shape'] = rotated_shape\n",
    "            return True\n",
    "        return False\n",
    "\n",
    "    def move_piece(self, dx, dy):\n",
    "        if self.valid_move(self.current_piece, x_offset=dx, y_offset=dy):\n",
    "            self.current_piece['x'] += dx\n",
    "            self.current_piece['y'] += dy\n",
    "            return True\n",
    "        return False\n",
    "\n",
    "    def lock_piece(self):\n",
    "        for y, row in enumerate(self.current_piece['shape']):\n",
    "            for x, cell in enumerate(row):\n",
    "                if cell:\n",
    "                    grid_y = self.current_piece['y'] + y\n",
    "                    grid_x = self.current_piece['x'] + x\n",
    "                    if grid_y >= 0:\n",
    "                        color_idx = SHAPE_COLORS.index(self.current_piece['color']) + 1\n",
    "                        self.grid[grid_y, grid_x] = color_idx\n",
    "        \n",
    "        self.pieces_placed += 1\n",
    "        lines = self.clear_lines()\n",
    "        \n",
    "        self.current_piece = self.next_piece\n",
    "        self.next_piece = self.new_piece()\n",
    "        \n",
    "        if not self.valid_move(self.current_piece):\n",
    "            self.game_over = True\n",
    "        \n",
    "        return lines\n",
    "\n",
    "    def clear_lines(self):\n",
    "        lines_cleared = 0\n",
    "        y = GRID_HEIGHT - 1\n",
    "        while y >= 0:\n",
    "            if all(self.grid[y, :]):\n",
    "                lines_cleared += 1\n",
    "                for y2 in range(y, 0, -1):\n",
    "                    self.grid[y2] = self.grid[y2 - 1]\n",
    "                self.grid[0] = np.zeros(GRID_WIDTH, dtype=int)\n",
    "            else:\n",
    "                y -= 1\n",
    "\n",
    "        self.lines_cleared += lines_cleared\n",
    "        if lines_cleared == 1:\n",
    "            self.score += 100\n",
    "        elif lines_cleared == 2:\n",
    "            self.score += 300\n",
    "        elif lines_cleared == 3:\n",
    "            self.score += 500\n",
    "        elif lines_cleared == 4:\n",
    "            self.score += 800\n",
    "\n",
    "        return lines_cleared\n",
    "\n",
    "    def update(self, delta_time):\n",
    "        if self.game_over:\n",
    "            return\n",
    "\n",
    "        self.fall_time += delta_time\n",
    "        if self.fall_time >= self.fall_speed:\n",
    "            self.fall_time = 0\n",
    "            if not self.move_piece(0, 1):\n",
    "                self.lock_piece()\n",
    "\n",
    "    def draw(self, surface):\n",
    "        surface.fill(BLACK)\n",
    "        \n",
    "        # Draw grid background\n",
    "        for y in range(GRID_HEIGHT):\n",
    "            for x in range(GRID_WIDTH):\n",
    "                pygame.draw.rect(surface, DARK_GRAY, \n",
    "                                (x * BLOCK_SIZE, y * BLOCK_SIZE, BLOCK_SIZE, BLOCK_SIZE))\n",
    "                pygame.draw.rect(surface, GRAY, \n",
    "                                (x * BLOCK_SIZE, y * BLOCK_SIZE, BLOCK_SIZE, BLOCK_SIZE), 1)\n",
    "        \n",
    "        # Draw placed blocks\n",
    "        for y in range(GRID_HEIGHT):\n",
    "            for x in range(GRID_WIDTH):\n",
    "                if self.grid[y, x]:\n",
    "                    color_idx = self.grid[y, x] - 1\n",
    "                    pygame.draw.rect(surface, SHAPE_COLORS[color_idx],\n",
    "                                     (x * BLOCK_SIZE, y * BLOCK_SIZE, BLOCK_SIZE, BLOCK_SIZE))\n",
    "                    pygame.draw.rect(surface, WHITE,\n",
    "                                     (x * BLOCK_SIZE, y * BLOCK_SIZE, BLOCK_SIZE, BLOCK_SIZE), 1)\n",
    "\n",
    "        # Draw current piece\n",
    "        if not self.game_over:\n",
    "            for y, row in enumerate(self.current_piece['shape']):\n",
    "                for x, cell in enumerate(row):\n",
    "                    if cell:\n",
    "                        pygame.draw.rect(surface, self.current_piece['color'],\n",
    "                                         ((self.current_piece['x'] + x) * BLOCK_SIZE,\n",
    "                                          (self.current_piece['y'] + y) * BLOCK_SIZE,\n",
    "                                          BLOCK_SIZE, BLOCK_SIZE))\n",
    "                        pygame.draw.rect(surface, WHITE,\n",
    "                                         ((self.current_piece['x'] + x) * BLOCK_SIZE,\n",
    "                                          (self.current_piece['y'] + y) * BLOCK_SIZE,\n",
    "                                          BLOCK_SIZE, BLOCK_SIZE), 1)\n",
    "\n",
    "        # Draw grid lines\n",
    "        for x in range(GRID_WIDTH + 1):\n",
    "            pygame.draw.line(surface, GRAY, (x * BLOCK_SIZE, 0), (x * BLOCK_SIZE, SCREEN_HEIGHT))\n",
    "        for y in range(GRID_HEIGHT + 1):\n",
    "            pygame.draw.line(surface, GRAY, (0, y * BLOCK_SIZE), (GRID_WIDTH * BLOCK_SIZE, y * BLOCK_SIZE))\n",
    "\n",
    "        # Draw sidebar\n",
    "        sidebar_x = GRID_WIDTH * BLOCK_SIZE + 5\n",
    "        pygame.draw.line(surface, WHITE, (sidebar_x, 0), (sidebar_x, SCREEN_HEIGHT))\n",
    "\n",
    "        # Draw next piece\n",
    "        font = pygame.font.SysFont(None, 24)\n",
    "        next_text = font.render(\"Next Piece:\", True, WHITE)\n",
    "        surface.blit(next_text, (sidebar_x + 10, 20))\n",
    "        \n",
    "        if not self.game_over:\n",
    "            next_piece_x = sidebar_x + 30\n",
    "            next_piece_y = 60\n",
    "            for y, row in enumerate(self.next_piece['shape']):\n",
    "                for x, cell in enumerate(row):\n",
    "                    if cell:\n",
    "                        pygame.draw.rect(surface, self.next_piece['color'],\n",
    "                                         (next_piece_x + x * BLOCK_SIZE, \n",
    "                                          next_piece_y + y * BLOCK_SIZE,\n",
    "                                          BLOCK_SIZE, BLOCK_SIZE))\n",
    "                        pygame.draw.rect(surface, WHITE,\n",
    "                                         (next_piece_x + x * BLOCK_SIZE, \n",
    "                                          next_piece_y + y * BLOCK_SIZE,\n",
    "                                          BLOCK_SIZE, BLOCK_SIZE), 1)\n",
    "\n",
    "        # Draw stats\n",
    "        score_text = font.render(f\"Score: {self.score}\", True, WHITE)\n",
    "        surface.blit(score_text, (sidebar_x + 10, 120))\n",
    "        \n",
    "        lines_text = font.render(f\"Lines: {self.lines_cleared}\", True, WHITE)\n",
    "        surface.blit(lines_text, (sidebar_x + 10, 150))\n",
    "        \n",
    "        pieces_text = font.render(f\"Pieces: {self.pieces_placed}\", True, WHITE)\n",
    "        surface.blit(pieces_text, (sidebar_x + 10, 180))\n",
    "        \n",
    "        # Draw last action\n",
    "        if self.last_action is not None:\n",
    "            action_text = font.render(f\"Action: {ACTION_NAMES.get(self.last_action, 'Unknown')}\", True, YELLOW)\n",
    "            surface.blit(action_text, (sidebar_x + 10, 220))\n",
    "\n",
    "        if self.game_over:\n",
    "            font = pygame.font.SysFont(None, 48)\n",
    "            game_over_text = font.render(\"GAME OVER\", True, RED)\n",
    "            text_rect = game_over_text.get_rect(center=(SCREEN_WIDTH // 2, SCREEN_HEIGHT // 2 - 24))\n",
    "            surface.blit(game_over_text, text_rect)\n",
    "\n",
    "def train_headless(num_episodes=1000, save_every=100, model_path='tetris_dqn.pth'):\n",
    "    \"\"\"Train the DQN agent in headless mode\"\"\"\n",
    "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "    print(f\"Training on device: {device}\")\n",
    "\n",
    "    # Initialize agent\n",
    "    state_size = 15  # 10 heights + holes + bumpiness + total_height + max_height + complete_lines\n",
    "    agent = DQNAgent(state_size, NUM_ACTIONS, device)\n",
    "\n",
    "    # Load existing model if available\n",
    "    if os.path.exists(model_path):\n",
    "        try:\n",
    "            agent.load(model_path)\n",
    "            print(f\"Loaded existing model from {model_path}\")\n",
    "        except:\n",
    "            print(\"Could not load model, starting fresh\")\n",
    "\n",
    "    # Training stats\n",
    "    scores = []\n",
    "    lines_cleared_list = []\n",
    "    pieces_placed_list = []\n",
    "\n",
    "    print(f\"\\nStarting training for {num_episodes} episodes...\")\n",
    "    print(\"=\" * 60)\n",
    "\n",
    "    for episode in range(num_episodes):\n",
    "        game = Tetris()\n",
    "        state = agent.get_state_features(game)\n",
    "        episode_reward = 0\n",
    "        steps = 0\n",
    "        \n",
    "        while not game.game_over:\n",
    "            # Select and perform action\n",
    "            action = agent.select_action(state, training=True)\n",
    "            game.last_action = action\n",
    "            \n",
    "            prev_score = game.score\n",
    "            prev_lines = game.lines_cleared\n",
    "            prev_height = max([sum(1 for row in range(GRID_HEIGHT) if game.grid[row, col] != 0) \n",
    "                             for col in range(GRID_WIDTH)])\n",
    "            \n",
    "            # Execute action\n",
    "            if action == ACTION_MOVE_LEFT:\n",
    "                game.move_piece(-1, 0)\n",
    "            elif action == ACTION_MOVE_RIGHT:\n",
    "                game.move_piece(1, 0)\n",
    "            elif action == ACTION_ROTATE:\n",
    "                game.rotate_piece()\n",
    "            elif action == ACTION_HARD_DROP:\n",
    "                while game.move_piece(0, 1):\n",
    "                    pass\n",
    "                lines = game.lock_piece()\n",
    "            \n",
    "            # Auto-drop every few steps\n",
    "            if steps % 5 == 0:\n",
    "                if not game.move_piece(0, 1):\n",
    "                    if action != ACTION_HARD_DROP:\n",
    "                        game.lock_piece()\n",
    "            \n",
    "            # Calculate reward\n",
    "            reward = 0\n",
    "            lines_cleared_now = game.lines_cleared - prev_lines\n",
    "            if lines_cleared_now > 0:\n",
    "                reward += lines_cleared_now ** 2 * 100  # Reward clearing lines heavily\n",
    "            \n",
    "            reward += (game.score - prev_score) * 0.1  # Small reward for score increase\n",
    "            \n",
    "            # Penalty for height\n",
    "            curr_height = max([sum(1 for row in range(GRID_HEIGHT) if game.grid[row, col] != 0) \n",
    "                             for col in range(GRID_WIDTH)])\n",
    "            if curr_height > prev_height:\n",
    "                reward -= (curr_height - prev_height) * 2\n",
    "            \n",
    "            # Game over penalty\n",
    "            if game.game_over:\n",
    "                reward -= 100\n",
    "            \n",
    "            # Get next state\n",
    "            next_state = agent.get_state_features(game)\n",
    "            \n",
    "            # Store transition\n",
    "            agent.memory.push(state, action, reward, next_state, game.game_over)\n",
    "            \n",
    "            # Train\n",
    "            loss = agent.train_step()\n",
    "            \n",
    "            episode_reward += reward\n",
    "            state = next_state\n",
    "            steps += 1\n",
    "        \n",
    "        # Record stats\n",
    "        scores.append(game.score)\n",
    "        lines_cleared_list.append(game.lines_cleared)\n",
    "        pieces_placed_list.append(game.pieces_placed)\n",
    "        \n",
    "        # Print progress\n",
    "        if (episode + 1) % 10 == 0:\n",
    "            avg_score = np.mean(scores[-10:])\n",
    "            avg_lines = np.mean(lines_cleared_list[-10:])\n",
    "            avg_pieces = np.mean(pieces_placed_list[-10:])\n",
    "            print(f\"Episode {episode + 1}/{num_episodes} | \"\n",
    "                  f\"Avg Score: {avg_score:.1f} | \"\n",
    "                  f\"Avg Lines: {avg_lines:.1f} | \"\n",
    "                  f\"Avg Pieces: {avg_pieces:.1f} | \"\n",
    "                  f\"Epsilon: {agent.epsilon:.3f}\")\n",
    "        \n",
    "        # Save model\n",
    "        if (episode + 1) % save_every == 0:\n",
    "            agent.save(model_path)\n",
    "            print(f\"Model saved to {model_path}\")\n",
    "\n",
    "    # Final save\n",
    "    agent.save(model_path)\n",
    "    print(f\"\\nTraining complete! Final model saved to {model_path}\")\n",
    "    print(\"=\" * 60)\n",
    "    print(f\"Final stats (last 100 episodes):\")\n",
    "    print(f\"  Average Score: {np.mean(scores[-100:]):.1f}\")\n",
    "    print(f\"  Average Lines: {np.mean(lines_cleared_list[-100:]):.1f}\")\n",
    "    print(f\"  Average Pieces: {np.mean(pieces_placed_list[-100:]):.1f}\")\n",
    "\n",
    "def play_with_trained_model(model_path='tetris_dqn.pth'):\n",
    "    \"\"\"Play Tetris with the trained model\"\"\"\n",
    "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "    state_size = 15\n",
    "    agent = DQNAgent(state_size, NUM_ACTIONS, device)\n",
    "\n",
    "    # Load model\n",
    "    if os.path.exists(model_path):\n",
    "        agent.load(model_path)\n",
    "        agent.epsilon = 0  # No exploration during play\n",
    "        print(f\"Loaded model from {model_path}\")\n",
    "    else:\n",
    "        print(f\"No model found at {model_path}\")\n",
    "        return\n",
    "\n",
    "    screen = pygame.display.set_mode((SCREEN_WIDTH, SCREEN_HEIGHT))\n",
    "    pygame.display.set_caption(\"Tetris DQN AI\")\n",
    "    clock = pygame.time.Clock()\n",
    "\n",
    "    game = Tetris()\n",
    "    running = True\n",
    "    ai_active = True\n",
    "\n",
    "    while running:\n",
    "        delta_time = clock.tick(60)\n",
    "        \n",
    "        for event in pygame.event.get():\n",
    "            if event.type == pygame.QUIT:\n",
    "                running = False\n",
    "            if event.type == pygame.KEYDOWN:\n",
    "                if event.key == pygame.K_r:\n",
    "                    game = Tetris()\n",
    "                elif event.key == pygame.K_a:\n",
    "                    ai_active = not ai_active\n",
    "        \n",
    "        # AI control\n",
    "        if ai_active and not game.game_over:\n",
    "            if pygame.time.get_ticks() % 10 == 0:\n",
    "                state = agent.get_state_features(game)\n",
    "                action = agent.select_action(state, training=False)\n",
    "                game.last_action = action\n",
    "                \n",
    "                if action == ACTION_MOVE_LEFT:\n",
    "                    game.move_piece(-1, 0)\n",
    "                elif action == ACTION_MOVE_RIGHT:\n",
    "                    game.move_piece(1, 0)\n",
    "                elif action == ACTION_ROTATE:\n",
    "                    game.rotate_piece()\n",
    "                elif action == ACTION_HARD_DROP:\n",
    "                    while game.move_piece(0, 1):\n",
    "                        pass\n",
    "                    game.lock_piece()\n",
    "        \n",
    "        game.update(delta_time)\n",
    "        game.draw(screen)\n",
    "        \n",
    "        # Display AI info\n",
    "        font = pygame.font.SysFont(None, 24)\n",
    "        ai_status = f\"DQN AI: {'ACTIVE' if ai_active else 'PAUSED'} (Press A)\"\n",
    "        status_text = font.render(ai_status, True, GREEN if ai_active else RED)\n",
    "        screen.blit(status_text, (10, SCREEN_HEIGHT - 60))\n",
    "        \n",
    "        small_font = pygame.font.SysFont(None, 18)\n",
    "        controls = [\n",
    "            \"CONTROLS:\",\n",
    "            \"A: Toggle AI\",\n",
    "            \"R: Reset Game\"\n",
    "        ]\n",
    "        for i, text in enumerate(controls):\n",
    "            help_text = small_font.render(text, True, WHITE)\n",
    "            screen.blit(help_text, (10, 10 + i * 20))\n",
    "        \n",
    "        pygame.display.flip()\n",
    "\n",
    "    pygame.quit()\n",
    "\n",
    "def main():\n",
    "    import sys\n",
    "\n",
    "    if len(sys.argv) > 1 and sys.argv[1] == 'train':\n",
    "        # Training mode\n",
    "        num_episodes = int(sys.argv[2]) if len(sys.argv) > 2 else 1000\n",
    "        train_headless(num_episodes=num_episodes)\n",
    "    else:\n",
    "        # Play mode\n",
    "        play_with_trained_model()\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "460cb7c0-3ceb-4e3b-bb7f-088159c73de3",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
