{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Module 5: Full Training Pipeline and Story Generation\n",
    "\n",
    "This is the capstone notebook where we bring everything together! We'll train a complete MoE-based story generation model and use it to create stories.\n",
    "\n",
    "## Learning Objectives\n",
    "\n",
    "By the end of this notebook, you will:\n",
    "1. Set up the complete Storyteller model with MoE\n",
    "2. Load and prepare real story datasets\n",
    "3. Configure MLflow for experiment tracking\n",
    "4. Train the full model end-to-end\n",
    "5. Monitor training with MLflow UI\n",
    "6. Implement text generation (sampling strategies)\n",
    "7. Generate creative stories with your trained model\n",
    "8. Evaluate and compare different checkpoints\n",
    "\n",
    "## What We'll Build\n",
    "\n",
    "- Complete MoE transformer for story generation\n",
    "- Production-ready training pipeline\n",
    "- MLflow experiment tracking integration\n",
    "- Multiple generation strategies (greedy, sampling, top-k, nucleus)\n",
    "- Interactive story generation interface"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "from pathlib import Path\n",
    "\n",
    "# Add src to path so we can import our modules\n",
    "project_root = Path.cwd().parent\n",
    "sys.path.insert(0, str(project_root / \"src\"))\n",
    "\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import yaml\n",
    "import mlflow\n",
    "from datetime import datetime\n",
    "\n",
    "# Import our modules\n",
    "from storyteller.model.transformer import StorytellerModel, TransformerConfig\n",
    "from storyteller.data.dataset import TextDataset\n",
    "from storyteller.training.trainer import Trainer\n",
    "from storyteller.inference.generator import Generator, GenerationConfig\n",
    "\n",
    "# Set seeds\n",
    "torch.manual_seed(42)\n",
    "np.random.seed(42)\n",
    "\n",
    "# Configure plotting\n",
    "sns.set_style(\"whitegrid\")\n",
    "plt.rcParams[\"figure.figsize\"] = (14, 6)\n",
    "\n",
    "# Device setup\n",
    "device = (\n",
    "    \"mps\"\n",
    "    if torch.backends.mps.is_available()\n",
    "    else \"cuda\"\n",
    "    if torch.cuda.is_available()\n",
    "    else \"cpu\"\n",
    ")\n",
    "print(f\"Using device: {device}\")\n",
    "print(f\"PyTorch version: {torch.__version__}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 1: Configuration\n",
    "\n",
    "Let's load our model configuration from the YAML file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load configuration\n",
    "config_path = project_root / \"configs\" / \"moe_model.yaml\"\n",
    "\n",
    "with open(config_path, \"r\") as f:\n",
    "    config = yaml.safe_load(f)\n",
    "\n",
    "print(\"Model Configuration:\")\n",
    "print(yaml.dump(config[\"model\"], default_flow_style=False))\n",
    "\n",
    "print(\"\\nTraining Configuration:\")\n",
    "print(yaml.dump(config[\"training\"], default_flow_style=False))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Understanding Our MoE Configuration\n",
    "\n",
    "Key parameters:\n",
    "- **Total parameters**: ~500M\n",
    "- **Active per token**: ~100M (using 2 of 8 experts)\n",
    "- **Layers**: 16 transformer blocks\n",
    "- **Hidden size**: 1024\n",
    "- **MoE frequency**: Every 2 layers (8 total MoE layers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create model config\n",
    "model_config = TransformerConfig(\n",
    "    vocab_size=config[\"model\"][\"vocab_size\"],\n",
    "    max_seq_length=config[\"model\"][\"max_seq_length\"],\n",
    "    hidden_size=config[\"model\"][\"hidden_size\"],\n",
    "    num_layers=config[\"model\"][\"num_layers\"],\n",
    "    num_attention_heads=config[\"model\"][\"num_attention_heads\"],\n",
    "    intermediate_size=config[\"model\"][\"intermediate_size\"],\n",
    "    attention_dropout=config[\"model\"][\"attention_dropout\"],\n",
    "    hidden_dropout=config[\"model\"][\"hidden_dropout\"],\n",
    "    positional_encoding=config[\"model\"][\"positional_encoding\"],\n",
    "    activation=config[\"model\"][\"activation\"],\n",
    "    gradient_checkpointing=config[\"model\"][\"gradient_checkpointing\"],\n",
    "    use_moe=config[\"model\"][\"use_moe\"],\n",
    "    num_experts=config[\"model\"][\"num_experts\"],\n",
    "    top_k_experts=config[\"model\"][\"top_k_experts\"],\n",
    "    moe_frequency=config[\"model\"][\"moe_frequency\"],\n",
    "    expert_capacity_factor=config[\"model\"][\"expert_capacity_factor\"],\n",
    "    load_balancing_loss_weight=config[\"model\"][\"load_balancing_loss_weight\"],\n",
    ")\n",
    "\n",
    "print(\"Model Config created successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 2: Data Preparation\n",
    "\n",
    "Load the datasets we prepared in Module 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Paths to data\n",
    "data_dir = project_root / \"data\" / \"processed\"\n",
    "train_data_path = data_dir / \"train.txt\"\n",
    "val_data_path = data_dir / \"val.txt\"\n",
    "\n",
    "# Check if data exists\n",
    "if not train_data_path.exists():\n",
    "    print(\"‚ö†Ô∏è  Training data not found!\")\n",
    "    print(\"Please run the data preparation scripts first:\")\n",
    "    print(\"  1. storyteller-download\")\n",
    "    print(\"  2. storyteller-tokenizer\")\n",
    "    print(\"  3. storyteller-preprocess\")\n",
    "else:\n",
    "    print(f\"‚úì Training data found: {train_data_path}\")\n",
    "    print(f\"‚úì Validation data found: {val_data_path}\")\n",
    "\n",
    "    # Create datasets\n",
    "    train_dataset = TextDataset(\n",
    "        data_path=str(train_data_path),\n",
    "        max_seq_length=config[\"model\"][\"max_seq_length\"],\n",
    "    )\n",
    "\n",
    "    val_dataset = TextDataset(\n",
    "        data_path=str(val_data_path),\n",
    "        max_seq_length=config[\"model\"][\"max_seq_length\"],\n",
    "    )\n",
    "\n",
    "    print(\"\\nDataset sizes:\")\n",
    "    print(f\"  Training examples: {len(train_dataset):,}\")\n",
    "    print(f\"  Validation examples: {len(val_dataset):,}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create DataLoaders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create dataloaders\n",
    "train_loader = DataLoader(\n",
    "    train_dataset,\n",
    "    batch_size=config[\"training\"][\"batch_size\"],\n",
    "    shuffle=True,\n",
    "    num_workers=config[\"training\"][\"num_workers\"],\n",
    "    pin_memory=config[\"training\"][\"pin_memory\"],\n",
    ")\n",
    "\n",
    "val_loader = DataLoader(\n",
    "    val_dataset,\n",
    "    batch_size=config[\"training\"][\"batch_size\"],\n",
    "    shuffle=False,\n",
    "    num_workers=config[\"training\"][\"num_workers\"],\n",
    "    pin_memory=config[\"training\"][\"pin_memory\"],\n",
    ")\n",
    "\n",
    "effective_batch_size = (\n",
    "    config[\"training\"][\"batch_size\"] * config[\"training\"][\"gradient_accumulation_steps\"]\n",
    ")\n",
    "\n",
    "print(\"DataLoaders created:\")\n",
    "print(f\"  Batch size: {config['training']['batch_size']}\")\n",
    "print(f\"  Gradient accumulation: {config['training']['gradient_accumulation_steps']}\")\n",
    "print(f\"  Effective batch size: {effective_batch_size}\")\n",
    "print(f\"  Training batches per epoch: {len(train_loader):,}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 3: Model Initialization\n",
    "\n",
    "Create our MoE Storyteller model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create model\n",
    "model = StorytellerModel(model_config)\n",
    "\n",
    "# Count parameters\n",
    "total_params = sum(p.numel() for p in model.parameters())\n",
    "trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "\n",
    "# Estimate active parameters (approximate)\n",
    "# For MoE: use top_k/num_experts ratio for MoE layers\n",
    "moe_layers = config[\"model\"][\"num_layers\"] // config[\"model\"][\"moe_frequency\"]\n",
    "dense_layers = config[\"model\"][\"num_layers\"] - moe_layers\n",
    "expert_activation_ratio = (\n",
    "    config[\"model\"][\"top_k_experts\"] / config[\"model\"][\"num_experts\"]\n",
    ")\n",
    "\n",
    "print(\"Model Architecture:\")\n",
    "print(f\"  Total layers: {config['model']['num_layers']}\")\n",
    "print(f\"  MoE layers: {moe_layers}\")\n",
    "print(f\"  Dense layers: {dense_layers}\")\n",
    "print(f\"  Experts per MoE layer: {config['model']['num_experts']}\")\n",
    "print(f\"  Active experts per token: {config['model']['top_k_experts']}\")\n",
    "print(\"\\nParameter Counts:\")\n",
    "print(f\"  Total parameters: {total_params:,} ({total_params / 1e6:.1f}M)\")\n",
    "print(f\"  Trainable parameters: {trainable_params:,} ({trainable_params / 1e6:.1f}M)\")\n",
    "print(\n",
    "    f\"  Approximate active per forward: ~{int(total_params * expert_activation_ratio):,} \"\n",
    "    f\"({total_params * expert_activation_ratio / 1e6:.1f}M)\"\n",
    ")\n",
    "print(f\"\\nModel size: ~{total_params * 4 / 1e9:.2f} GB (float32)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 4: MLflow Setup\n",
    "\n",
    "Configure experiment tracking with MLflow."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set MLflow tracking URI\n",
    "mlflow_uri = config[\"training\"].get(\"mlflow_tracking_uri\", \"http://localhost:8080\")\n",
    "mlflow.set_tracking_uri(mlflow_uri)\n",
    "\n",
    "# Set experiment\n",
    "experiment_name = config[\"training\"][\"mlflow_experiment_name\"]\n",
    "mlflow.set_experiment(experiment_name)\n",
    "\n",
    "print(\"MLflow Configuration:\")\n",
    "print(f\"  Tracking URI: {mlflow_uri}\")\n",
    "print(f\"  Experiment: {experiment_name}\")\n",
    "print(\"\\nTo view experiments, run in terminal:\")\n",
    "print(\"  mlflow ui --port 8080\")\n",
    "print(\"Then open: http://localhost:8080\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 5: Create Trainer\n",
    "\n",
    "Set up the complete training pipeline with our Trainer class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create optimizer\n",
    "optimizer = torch.optim.AdamW(\n",
    "    model.parameters(),\n",
    "    lr=config[\"training\"][\"learning_rate\"],\n",
    "    weight_decay=config[\"training\"][\"weight_decay\"],\n",
    "    betas=(0.9, 0.95),\n",
    ")\n",
    "\n",
    "# Create learning rate scheduler\n",
    "from torch.optim.lr_scheduler import CosineAnnealingLR\n",
    "\n",
    "num_training_steps = len(train_loader) * config[\"training\"][\"num_epochs\"]\n",
    "num_training_steps //= config[\"training\"][\"gradient_accumulation_steps\"]\n",
    "\n",
    "scheduler = CosineAnnealingLR(\n",
    "    optimizer,\n",
    "    T_max=num_training_steps - config[\"training\"][\"warmup_steps\"],\n",
    "    eta_min=config[\"training\"][\"learning_rate\"] * 0.1,\n",
    ")\n",
    "\n",
    "print(\"Optimizer: AdamW\")\n",
    "print(f\"  Learning rate: {config['training']['learning_rate']:.2e}\")\n",
    "print(f\"  Weight decay: {config['training']['weight_decay']}\")\n",
    "print(\"\\nScheduler: Cosine Annealing\")\n",
    "print(f\"  Total steps: {num_training_steps:,}\")\n",
    "print(f\"  Warmup steps: {config['training']['warmup_steps']:,}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create trainer\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    train_dataloader=train_loader,\n",
    "    val_dataloader=val_loader,\n",
    "    optimizer=optimizer,\n",
    "    scheduler=scheduler,\n",
    "    device=config[\"training\"][\"device\"],\n",
    "    use_amp=config[\"training\"][\"use_amp\"],\n",
    "    amp_dtype=config[\"training\"][\"amp_dtype\"],\n",
    "    gradient_accumulation_steps=config[\"training\"][\"gradient_accumulation_steps\"],\n",
    "    max_grad_norm=config[\"training\"][\"max_grad_norm\"],\n",
    "    save_dir=config[\"training\"][\"save_dir\"],\n",
    "    save_every_n_steps=config[\"training\"][\"save_every_n_steps\"],\n",
    "    eval_every_n_steps=config[\"training\"][\"eval_every_n_steps\"],\n",
    "    log_every_n_steps=config[\"training\"][\"log_every_n_steps\"],\n",
    "    keep_last_n_checkpoints=config[\"training\"][\"keep_last_n_checkpoints\"],\n",
    "    use_mlflow=config[\"training\"][\"use_mlflow\"],\n",
    "    mlflow_experiment_name=config[\"training\"][\"mlflow_experiment_name\"],\n",
    "    mlflow_run_name=config[\"training\"][\"mlflow_run_name\"],\n",
    "    mlflow_tracking_uri=config[\"training\"].get(\"mlflow_tracking_uri\"),\n",
    ")\n",
    "\n",
    "print(\"‚úì Trainer created successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 6: Training!\n",
    "\n",
    "Now let's train our model. This may take a while depending on your hardware.\n",
    "\n",
    "**Note**: For a full training run, you'd typically train for many more epochs. This is a demonstration."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Start training\n",
    "print(\"=\" * 60)\n",
    "print(\"STARTING TRAINING\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "try:\n",
    "    trainer.train(num_epochs=config[\"training\"][\"num_epochs\"])\n",
    "    print(\"\\n‚úì Training completed successfully!\")\n",
    "except KeyboardInterrupt:\n",
    "    print(\"\\n‚ö†Ô∏è  Training interrupted by user\")\n",
    "    # Save checkpoint\n",
    "    trainer.save_checkpoint(\"interrupted.pt\")\n",
    "except Exception as e:\n",
    "    print(f\"\\n‚ùå Training failed with error: {e}\")\n",
    "    raise"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 7: Viewing Training Metrics\n",
    "\n",
    "Let's visualize the training progress."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# If MLflow is running, you can fetch metrics programmatically\n",
    "try:\n",
    "    # Get current run\n",
    "    run = mlflow.active_run()\n",
    "    if run:\n",
    "        client = mlflow.tracking.MlflowClient()\n",
    "\n",
    "        # Get metrics\n",
    "        metrics = client.get_run(run.info.run_id).data.metrics\n",
    "\n",
    "        print(\"Training Metrics (final values):\")\n",
    "        for key, value in sorted(metrics.items()):\n",
    "            print(f\"  {key}: {value:.4f}\")\n",
    "except Exception as e:\n",
    "    print(f\"Could not fetch MLflow metrics: {e}\")\n",
    "    print(\"Check MLflow UI for detailed metrics and visualizations\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 8: Text Generation\n",
    "\n",
    "Now for the fun part - let's generate stories!\n",
    "\n",
    "### Generation Strategies\n",
    "\n",
    "1. **Greedy**: Always pick highest probability token (deterministic, boring)\n",
    "2. **Sampling**: Sample from probability distribution (random, diverse)\n",
    "3. **Top-K**: Sample from top K most likely tokens\n",
    "4. **Nucleus (Top-P)**: Sample from smallest set with cumulative prob ‚â• p\n",
    "5. **Temperature**: Scale logits to control randomness"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load tokenizer\n",
    "from tokenizers import Tokenizer\n",
    "\n",
    "tokenizer_path = project_root / \"models\" / \"tokenizer\" / \"storyteller_tokenizer.json\"\n",
    "\n",
    "if tokenizer_path.exists():\n",
    "    tokenizer = Tokenizer.from_file(str(tokenizer_path))\n",
    "    print(f\"‚úì Tokenizer loaded from {tokenizer_path}\")\n",
    "    print(f\"  Vocabulary size: {tokenizer.get_vocab_size()}\")\n",
    "else:\n",
    "    print(\"‚ö†Ô∏è  Tokenizer not found! Please train tokenizer first.\")\n",
    "    tokenizer = None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create Generator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create generator\n",
    "generator = Generator(\n",
    "    model=model,\n",
    "    tokenizer=tokenizer,\n",
    "    device=device,\n",
    ")\n",
    "\n",
    "print(\"‚úì Generator created!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generate Stories with Different Strategies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Story prompt\n",
    "prompt = \"Once upon a time, in a land far away, there lived a\"\n",
    "\n",
    "print(f\"Prompt: '{prompt}'\")\n",
    "print(\"\\n\" + \"=\" * 70)\n",
    "\n",
    "# 1. Greedy decoding (deterministic)\n",
    "print(\"\\n1. GREEDY DECODING (deterministic)\")\n",
    "print(\"-\" * 70)\n",
    "greedy_config = GenerationConfig(\n",
    "    max_new_tokens=100,\n",
    "    temperature=1.0,\n",
    "    top_k=1,  # Greedy\n",
    "    top_p=1.0,\n",
    ")\n",
    "story = generator.generate(prompt, greedy_config)\n",
    "print(story)\n",
    "\n",
    "# 2. High temperature (creative/random)\n",
    "print(\"\\n\" + \"=\" * 70)\n",
    "print(\"\\n2. HIGH TEMPERATURE (creative, random)\")\n",
    "print(\"-\" * 70)\n",
    "creative_config = GenerationConfig(\n",
    "    max_new_tokens=100,\n",
    "    temperature=1.2,\n",
    "    top_k=50,\n",
    "    top_p=0.95,\n",
    ")\n",
    "story = generator.generate(prompt, creative_config)\n",
    "print(story)\n",
    "\n",
    "# 3. Low temperature (focused/coherent)\n",
    "print(\"\\n\" + \"=\" * 70)\n",
    "print(\"\\n3. LOW TEMPERATURE (focused, coherent)\")\n",
    "print(\"-\" * 70)\n",
    "focused_config = GenerationConfig(\n",
    "    max_new_tokens=100,\n",
    "    temperature=0.7,\n",
    "    top_k=40,\n",
    "    top_p=0.9,\n",
    ")\n",
    "story = generator.generate(prompt, focused_config)\n",
    "print(story)\n",
    "\n",
    "# 4. Nucleus sampling (balanced)\n",
    "print(\"\\n\" + \"=\" * 70)\n",
    "print(\"\\n4. NUCLEUS SAMPLING (balanced)\")\n",
    "print(\"-\" * 70)\n",
    "nucleus_config = GenerationConfig(\n",
    "    max_new_tokens=100,\n",
    "    temperature=0.9,\n",
    "    top_k=0,  # Disable top-k\n",
    "    top_p=0.95,\n",
    ")\n",
    "story = generator.generate(prompt, nucleus_config)\n",
    "print(story)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 9: Interactive Story Generation\n",
    "\n",
    "Create an interactive interface for generating stories."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_interactive_story():\n",
    "    \"\"\"\n",
    "    Interactive story generation with custom prompts.\n",
    "    \"\"\"\n",
    "    print(\"=\" * 70)\n",
    "    print(\"INTERACTIVE STORY GENERATOR\")\n",
    "    print(\"=\" * 70)\n",
    "    print(\"\\nEnter your story prompt (or 'quit' to exit)\")\n",
    "    print(\"Example: 'In a dark forest, a young wizard discovered'\")\n",
    "    print()\n",
    "\n",
    "    while True:\n",
    "        # Get prompt\n",
    "        prompt = input(\"\\nPrompt: \").strip()\n",
    "\n",
    "        if prompt.lower() in [\"quit\", \"exit\", \"q\"]:\n",
    "            print(\"Goodbye!\")\n",
    "            break\n",
    "\n",
    "        if not prompt:\n",
    "            print(\"Please enter a prompt!\")\n",
    "            continue\n",
    "\n",
    "        # Get generation parameters\n",
    "        print(\"\\nGeneration parameters:\")\n",
    "        try:\n",
    "            max_tokens = int(input(\"  Max tokens (default 150): \") or \"150\")\n",
    "            temperature = float(input(\"  Temperature 0.1-2.0 (default 0.9): \") or \"0.9\")\n",
    "            top_p = float(input(\"  Top-p 0.0-1.0 (default 0.95): \") or \"0.95\")\n",
    "        except ValueError:\n",
    "            print(\"Invalid input! Using defaults.\")\n",
    "            max_tokens, temperature, top_p = 150, 0.9, 0.95\n",
    "\n",
    "        # Generate\n",
    "        print(\"\\nGenerating...\")\n",
    "        gen_config = GenerationConfig(\n",
    "            max_new_tokens=max_tokens,\n",
    "            temperature=temperature,\n",
    "            top_p=top_p,\n",
    "            top_k=50,\n",
    "        )\n",
    "\n",
    "        story = generator.generate(prompt, gen_config)\n",
    "\n",
    "        print(\"\\n\" + \"=\" * 70)\n",
    "        print(\"GENERATED STORY:\")\n",
    "        print(\"=\" * 70)\n",
    "        print(story)\n",
    "        print(\"=\" * 70)\n",
    "\n",
    "\n",
    "# Uncomment to run interactive mode\n",
    "# generate_interactive_story()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 10: Evaluating Different Checkpoints\n",
    "\n",
    "Compare models from different training stages."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compare_checkpoints(checkpoint_paths, prompt, gen_config):\n",
    "    \"\"\"\n",
    "    Generate stories from multiple checkpoints for comparison.\n",
    "    \"\"\"\n",
    "    results = []\n",
    "\n",
    "    for ckpt_path in checkpoint_paths:\n",
    "        # Load checkpoint\n",
    "        checkpoint = torch.load(ckpt_path, map_location=device)\n",
    "        model.load_state_dict(checkpoint[\"model_state_dict\"])\n",
    "\n",
    "        # Generate\n",
    "        story = generator.generate(prompt, gen_config)\n",
    "\n",
    "        results.append(\n",
    "            {\n",
    "                \"checkpoint\": ckpt_path.name,\n",
    "                \"step\": checkpoint.get(\"global_step\", \"unknown\"),\n",
    "                \"val_loss\": checkpoint.get(\"best_val_loss\", \"unknown\"),\n",
    "                \"story\": story,\n",
    "            }\n",
    "        )\n",
    "\n",
    "    # Display results\n",
    "    print(\"=\" * 70)\n",
    "    print(\"CHECKPOINT COMPARISON\")\n",
    "    print(\"=\" * 70)\n",
    "    print(f\"\\nPrompt: '{prompt}'\\n\")\n",
    "\n",
    "    for i, result in enumerate(results, 1):\n",
    "        print(f\"\\n{i}. {result['checkpoint']}\")\n",
    "        print(f\"   Step: {result['step']}, Val Loss: {result['val_loss']}\")\n",
    "        print(\"-\" * 70)\n",
    "        print(result[\"story\"])\n",
    "        print(\"=\" * 70)\n",
    "\n",
    "\n",
    "# Example usage (uncomment when you have checkpoints)\n",
    "# checkpoint_dir = Path(config['training']['save_dir'])\n",
    "# checkpoints = list(checkpoint_dir.glob('checkpoint_step_*.pt'))\n",
    "#\n",
    "# if checkpoints:\n",
    "#     compare_checkpoints(\n",
    "#         checkpoints[:3],  # Compare first 3 checkpoints\n",
    "#         prompt=\"The brave knight ventured into\",\n",
    "#         gen_config=GenerationConfig(max_new_tokens=80, temperature=0.9),\n",
    "#     )\n",
    "# else:\n",
    "#     print(\"No checkpoints found yet!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 11: Understanding Generation Quality\n",
    "\n",
    "### What Makes Good Generated Text?\n",
    "\n",
    "1. **Coherence**: Logical flow, consistent context\n",
    "2. **Creativity**: Novel ideas, not just memorization\n",
    "3. **Fluency**: Natural language, proper grammar\n",
    "4. **Relevance**: Stays on topic from prompt\n",
    "\n",
    "### Hyperparameter Effects:\n",
    "\n",
    "- **Temperature**:\n",
    "  - Low (0.5-0.7): Safe, coherent, repetitive\n",
    "  - Medium (0.8-1.0): Balanced creativity\n",
    "  - High (1.2-2.0): Creative but may be nonsensical\n",
    "\n",
    "- **Top-P (Nucleus)**:\n",
    "  - Low (0.7-0.8): Conservative choices\n",
    "  - Medium (0.9-0.95): Good balance\n",
    "  - High (0.95-1.0): More diversity\n",
    "\n",
    "- **Top-K**:\n",
    "  - Small (10-20): Very focused\n",
    "  - Medium (40-50): Balanced\n",
    "  - Large (100+): More random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize the effect of temperature\n",
    "def plot_temperature_effects(logits, temperatures=[0.5, 1.0, 2.0]):\n",
    "    \"\"\"\n",
    "    Visualize how temperature affects probability distribution.\n",
    "    \"\"\"\n",
    "    fig, axes = plt.subplots(1, len(temperatures), figsize=(15, 4))\n",
    "\n",
    "    for idx, temp in enumerate(temperatures):\n",
    "        # Apply temperature\n",
    "        scaled_logits = logits / temp\n",
    "        probs = F.softmax(torch.tensor(scaled_logits), dim=-1).numpy()\n",
    "\n",
    "        # Plot\n",
    "        axes[idx].bar(range(len(probs)), probs, color=\"steelblue\", alpha=0.7)\n",
    "        axes[idx].set_title(f\"Temperature = {temp}\")\n",
    "        axes[idx].set_xlabel(\"Token\")\n",
    "        axes[idx].set_ylabel(\"Probability\")\n",
    "        axes[idx].set_ylim(0, max(probs) * 1.1)\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "# Example logits (10 tokens)\n",
    "example_logits = np.array([2.0, 1.5, 1.0, 0.5, 0.3, 0.1, -0.5, -1.0, -2.0, -3.0])\n",
    "plot_temperature_effects(example_logits)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary and Next Steps\n",
    "\n",
    "Congratulations! You've completed the full Storyteller training pipeline! üéâ\n",
    "\n",
    "### What You've Learned:\n",
    "\n",
    "1. **Complete Training Pipeline**:\n",
    "   - Data loading and preprocessing\n",
    "   - Model initialization (MoE transformer)\n",
    "   - Optimizer and scheduler configuration\n",
    "   - Mixed precision training\n",
    "   - MLflow experiment tracking\n",
    "\n",
    "2. **Text Generation**:\n",
    "   - Different sampling strategies\n",
    "   - Temperature and nucleus sampling\n",
    "   - Interactive generation\n",
    "   - Quality evaluation\n",
    "\n",
    "3. **Production Practices**:\n",
    "   - Checkpointing and model saving\n",
    "   - Experiment tracking with MLflow\n",
    "   - Hyperparameter tuning\n",
    "   - Model comparison\n",
    "\n",
    "### Next Steps for Improvement:\n",
    "\n",
    "1. **Training Improvements**:\n",
    "   - Train for more epochs (10-50)\n",
    "   - Experiment with learning rate schedules\n",
    "   - Try different optimizer settings\n",
    "   - Use larger/more diverse datasets\n",
    "\n",
    "2. **Model Improvements**:\n",
    "   - Scale up model size (more layers/larger hidden size)\n",
    "   - Adjust MoE parameters (num experts, top-k)\n",
    "   - Experiment with different architectures\n",
    "\n",
    "3. **Generation Improvements**:\n",
    "   - Implement beam search\n",
    "   - Add repetition penalty\n",
    "   - Try constrained generation\n",
    "   - Fine-tune on specific story genres\n",
    "\n",
    "4. **Evaluation**:\n",
    "   - Implement perplexity evaluation\n",
    "   - Human evaluation of generated stories\n",
    "   - Automatic metrics (BLEU, ROUGE)\n",
    "   - A/B testing different configurations\n",
    "\n",
    "### Resources:\n",
    "\n",
    "- **Model Architecture**: See `src/storyteller/model/`\n",
    "- **Training Code**: See `src/storyteller/training/`\n",
    "- **Generation Code**: See `src/storyteller/inference/`\n",
    "- **Configs**: See `configs/`\n",
    "- **MLflow**: View at http://localhost:8080\n",
    "\n",
    "### Community:\n",
    "\n",
    "Share your generated stories and improvements! This is an educational project - experiment, break things, and learn!\n",
    "\n",
    "Happy storytelling! üìö‚ú®"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bonus: Export Model for Production\n",
    "\n",
    "Export your trained model for deployment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def export_model_for_production(model, tokenizer, save_dir):\n",
    "    \"\"\"\n",
    "    Export model in production-ready format.\n",
    "    \"\"\"\n",
    "    save_dir = Path(save_dir)\n",
    "    save_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "    # Save model state dict\n",
    "    model_path = save_dir / \"model.pt\"\n",
    "    torch.save(\n",
    "        {\n",
    "            \"model_state_dict\": model.state_dict(),\n",
    "            \"model_config\": model.config.__dict__,\n",
    "        },\n",
    "        model_path,\n",
    "    )\n",
    "    print(f\"‚úì Model saved to {model_path}\")\n",
    "\n",
    "    # Save tokenizer\n",
    "    if tokenizer is not None:\n",
    "        tokenizer_path = save_dir / \"tokenizer.json\"\n",
    "        tokenizer.save(str(tokenizer_path))\n",
    "        print(f\"‚úì Tokenizer saved to {tokenizer_path}\")\n",
    "\n",
    "    # Save config\n",
    "    config_path = save_dir / \"config.yaml\"\n",
    "    with open(config_path, \"w\") as f:\n",
    "        yaml.dump(config, f)\n",
    "    print(f\"‚úì Config saved to {config_path}\")\n",
    "\n",
    "    # Save generation config template\n",
    "    gen_config_path = save_dir / \"generation_config.yaml\"\n",
    "    default_gen_config = {\n",
    "        \"max_new_tokens\": 512,\n",
    "        \"temperature\": 0.9,\n",
    "        \"top_p\": 0.95,\n",
    "        \"top_k\": 50,\n",
    "        \"repetition_penalty\": 1.1,\n",
    "    }\n",
    "    with open(gen_config_path, \"w\") as f:\n",
    "        yaml.dump(default_gen_config, f)\n",
    "    print(f\"‚úì Generation config saved to {gen_config_path}\")\n",
    "\n",
    "    print(f\"\\n‚úì Model exported successfully to {save_dir}\")\n",
    "    print(\"\\nTo use in production:\")\n",
    "    print(\"  1. Load model state dict\")\n",
    "    print(\"  2. Load tokenizer\")\n",
    "    print(\"  3. Initialize Generator\")\n",
    "    print(\"  4. Generate stories!\")\n",
    "\n",
    "\n",
    "# Export the trained model\n",
    "export_dir = (\n",
    "    project_root\n",
    "    / \"models\"\n",
    "    / \"production\"\n",
    "    / f\"storyteller_{datetime.now().strftime('%Y%m%d_%H%M%S')}\"\n",
    ")\n",
    "export_model_for_production(model, tokenizer, export_dir)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
