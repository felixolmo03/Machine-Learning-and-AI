{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Module 2: Understanding Transformers\n",
    "\n",
    "In this notebook, we'll build a deep understanding of transformer architectures from the ground up.\n",
    "\n",
    "## Learning Objectives\n",
    "\n",
    "By the end of this notebook, you will:\n",
    "1. Understand the core concepts behind self-attention\n",
    "2. Implement scaled dot-product attention from scratch\n",
    "3. Build multi-head attention mechanisms\n",
    "4. Explore positional encodings (learned vs. rotary)\n",
    "5. Understand the decoder-only architecture (GPT-style)\n",
    "6. Visualize attention patterns\n",
    "\n",
    "## Prerequisites\n",
    "\n",
    "- Basic understanding of neural networks\n",
    "- Familiarity with PyTorch\n",
    "- Completed Module 1 (Data Preparation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Set random seeds for reproducibility\n",
    "torch.manual_seed(42)\n",
    "np.random.seed(42)\n",
    "\n",
    "# Configure plotting\n",
    "sns.set_style(\"whitegrid\")\n",
    "plt.rcParams[\"figure.figsize\"] = (10, 6)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 1: The Attention Mechanism\n",
    "\n",
    "### What is Attention?\n",
    "\n",
    "Attention allows a model to focus on relevant parts of the input when processing each element. Think of it like reading a sentence - when you process the word \"it\", you look back at previous words to understand what \"it\" refers to.\n",
    "\n",
    "### Key Concepts:\n",
    "\n",
    "- **Query (Q)**: What am I looking for?\n",
    "- **Key (K)**: What do I have to offer?\n",
    "- **Value (V)**: What information do I actually contain?\n",
    "\n",
    "The attention mechanism computes a weighted sum of values, where weights are determined by the compatibility between queries and keys."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Scaled Dot-Product Attention\n",
    "\n",
    "The fundamental attention operation:\n",
    "\n",
    "$$\\text{Attention}(Q, K, V) = \\text{softmax}\\left(\\frac{QK^T}{\\sqrt{d_k}}\\right)V$$\n",
    "\n",
    "Where:\n",
    "- $d_k$ is the dimension of the key vectors\n",
    "- Division by $\\sqrt{d_k}$ prevents dot products from becoming too large\n",
    "- Softmax converts scores into a probability distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def scaled_dot_product_attention(query, key, value, mask=None):\n",
    "    \"\"\"\n",
    "    Compute scaled dot-product attention.\n",
    "\n",
    "    Args:\n",
    "        query: Query tensor of shape (batch_size, seq_len, d_k)\n",
    "        key: Key tensor of shape (batch_size, seq_len, d_k)\n",
    "        value: Value tensor of shape (batch_size, seq_len, d_v)\n",
    "        mask: Optional mask tensor (1 for positions to keep, 0 to mask)\n",
    "\n",
    "    Returns:\n",
    "        output: Attention output (batch_size, seq_len, d_v)\n",
    "        attention_weights: Attention weights (batch_size, seq_len, seq_len)\n",
    "    \"\"\"\n",
    "    d_k = query.size(-1)\n",
    "\n",
    "    # 1. Compute attention scores: Q @ K^T\n",
    "    scores = torch.matmul(query, key.transpose(-2, -1))  # (batch, seq_len, seq_len)\n",
    "\n",
    "    # 2. Scale by sqrt(d_k)\n",
    "    scores = scores / np.sqrt(d_k)\n",
    "\n",
    "    # 3. Apply mask (if provided) - set masked positions to -inf\n",
    "    if mask is not None:\n",
    "        scores = scores.masked_fill(mask == 0, float(\"-inf\"))\n",
    "\n",
    "    # 4. Apply softmax to get attention weights\n",
    "    attention_weights = F.softmax(scores, dim=-1)\n",
    "\n",
    "    # 5. Compute weighted sum of values\n",
    "    output = torch.matmul(attention_weights, value)\n",
    "\n",
    "    return output, attention_weights"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Let's Test It!\n",
    "\n",
    "We'll create a simple example with 4 tokens in a sequence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create sample input\n",
    "batch_size = 1\n",
    "seq_len = 4\n",
    "d_model = 8  # embedding dimension\n",
    "\n",
    "# Random query, key, value vectors\n",
    "Q = torch.randn(batch_size, seq_len, d_model)\n",
    "K = torch.randn(batch_size, seq_len, d_model)\n",
    "V = torch.randn(batch_size, seq_len, d_model)\n",
    "\n",
    "# Compute attention\n",
    "output, attn_weights = scaled_dot_product_attention(Q, K, V)\n",
    "\n",
    "print(f\"Input shape: {Q.shape}\")\n",
    "print(f\"Output shape: {output.shape}\")\n",
    "print(f\"Attention weights shape: {attn_weights.shape}\")\n",
    "print(\"\\nAttention weights (each row sums to 1):\")\n",
    "print(attn_weights[0].detach().numpy())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualizing Attention\n",
    "\n",
    "Let's visualize the attention pattern. Brighter colors indicate higher attention weights."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_attention(attention_weights, tokens=None):\n",
    "    \"\"\"\n",
    "    Visualize attention weights as a heatmap.\n",
    "    \"\"\"\n",
    "    # Convert to numpy\n",
    "    attn = attention_weights.detach().cpu().numpy()\n",
    "    if len(attn.shape) == 3:\n",
    "        attn = attn[0]  # Take first batch\n",
    "\n",
    "    # Create labels\n",
    "    if tokens is None:\n",
    "        tokens = [f\"Token {i}\" for i in range(attn.shape[0])]\n",
    "\n",
    "    # Plot\n",
    "    plt.figure(figsize=(10, 8))\n",
    "    sns.heatmap(\n",
    "        attn,\n",
    "        annot=True,\n",
    "        fmt=\".2f\",\n",
    "        cmap=\"Blues\",\n",
    "        xticklabels=tokens,\n",
    "        yticklabels=tokens,\n",
    "        cbar_kws={\"label\": \"Attention Weight\"},\n",
    "    )\n",
    "    plt.xlabel(\"Key/Value Position\")\n",
    "    plt.ylabel(\"Query Position\")\n",
    "    plt.title(\"Attention Pattern\")\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "# Visualize our attention weights\n",
    "plot_attention(attn_weights)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Causal Masking (for Decoder-only Models)\n",
    "\n",
    "In language models (like GPT), we predict the next token based only on previous tokens. This requires **causal masking** - preventing tokens from attending to future tokens."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_causal_mask(seq_len):\n",
    "    \"\"\"\n",
    "    Create a causal mask that prevents attention to future positions.\n",
    "\n",
    "    Returns a lower triangular matrix:\n",
    "    [[1, 0, 0, 0],\n",
    "     [1, 1, 0, 0],\n",
    "     [1, 1, 1, 0],\n",
    "     [1, 1, 1, 1]]\n",
    "    \"\"\"\n",
    "    mask = torch.tril(torch.ones(seq_len, seq_len))\n",
    "    return mask\n",
    "\n",
    "\n",
    "# Create and visualize causal mask\n",
    "mask = create_causal_mask(4)\n",
    "print(\"Causal Mask:\")\n",
    "print(mask.numpy())\n",
    "\n",
    "# Apply causal attention\n",
    "output_causal, attn_weights_causal = scaled_dot_product_attention(Q, K, V, mask=mask)\n",
    "\n",
    "print(\"\\nCausal Attention Weights:\")\n",
    "plot_attention(attn_weights_causal, tokens=[\"The\", \"cat\", \"sat\", \"down\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 2: Multi-Head Attention\n",
    "\n",
    "### Why Multiple Heads?\n",
    "\n",
    "Multi-head attention allows the model to attend to different aspects of the input simultaneously. Think of it like having multiple \"perspectives\" or \"experts\" looking at the same data.\n",
    "\n",
    "For example, when processing the sentence \"The cat sat on the mat\":\n",
    "- Head 1 might focus on subject-verb relationships\n",
    "- Head 2 might focus on positional relationships (on, under, etc.)\n",
    "- Head 3 might focus on noun attributes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "class MultiHeadAttention(nn.Module):\n    \"\"\"\n    Multi-Head Attention mechanism.\n    \"\"\"\n\n    def __init__(self, d_model, num_heads, dropout=0.1):\n        super().__init__()\n        assert d_model % num_heads == 0, \"d_model must be divisible by num_heads\"\n\n        self.d_model = d_model\n        self.num_heads = num_heads\n        self.d_k = d_model // num_heads  # Dimension per head\n\n        # Linear projections for Q, K, V\n        self.W_q = nn.Linear(d_model, d_model)\n        self.W_k = nn.Linear(d_model, d_model)\n        self.W_v = nn.Linear(d_model, d_model)\n\n        # Output projection\n        self.W_o = nn.Linear(d_model, d_model)\n\n        self.dropout = nn.Dropout(dropout)\n\n    def split_heads(self, x):\n        \"\"\"Split the last dimension into (num_heads, d_k).\"\"\"\n        batch_size, seq_len, d_model = x.size()\n        # Reshape: (batch, seq_len, num_heads, d_k)\n        x = x.view(batch_size, seq_len, self.num_heads, self.d_k)\n        # Transpose: (batch, num_heads, seq_len, d_k)\n        return x.transpose(1, 2)\n\n    def combine_heads(self, x):\n        \"\"\"Combine heads back to original dimension.\"\"\"\n        batch_size, num_heads, seq_len, d_k = x.size()\n        # Transpose: (batch, seq_len, num_heads, d_k)\n        x = x.transpose(1, 2).contiguous()\n        # Reshape: (batch, seq_len, d_model)\n        return x.view(batch_size, seq_len, self.d_model)\n\n    def forward(self, query, key, value, mask=None):\n        \"\"\"\n        Forward pass for multi-head attention.\n\n        Args:\n            query, key, value: Input tensors of shape (batch, seq_len, d_model)\n            mask: Optional attention mask\n        \"\"\"\n        # 1. Linear projections\n        Q = self.W_q(query)  # (batch, seq_len, d_model)\n        K = self.W_k(key)\n        V = self.W_v(value)\n\n        # 2. Split into multiple heads\n        Q = self.split_heads(Q)  # (batch, num_heads, seq_len, d_k)\n        K = self.split_heads(K)\n        V = self.split_heads(V)\n\n        # 3. Compute attention scores\n        scores = torch.matmul(Q, K.transpose(-2, -1)) / np.sqrt(self.d_k)\n\n        # 4. Apply mask\n        if mask is not None:\n            # Expand mask for multiple heads\n            mask = mask.unsqueeze(1)  # (batch, 1, seq_len, seq_len)\n            scores = scores.masked_fill(mask == 0, float(\"-inf\"))\n\n        # 5. Apply softmax and dropout\n        attention_weights = F.softmax(scores, dim=-1)\n        attention_weights = self.dropout(attention_weights)\n\n        # 6. Apply attention to values\n        context = torch.matmul(attention_weights, V)  # (batch, num_heads, seq_len, d_k)\n\n        # 7. Combine heads\n        context = self.combine_heads(context)  # (batch, seq_len, d_model)\n\n        # 8. Final linear projection\n        output = self.W_o(context)\n\n        return output, attention_weights"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Testing Multi-Head Attention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create multi-head attention module\n",
    "d_model = 512\n",
    "num_heads = 8\n",
    "seq_len = 10\n",
    "batch_size = 2\n",
    "\n",
    "mha = MultiHeadAttention(d_model, num_heads)\n",
    "\n",
    "# Create sample input\n",
    "x = torch.randn(batch_size, seq_len, d_model)\n",
    "mask = create_causal_mask(seq_len).unsqueeze(0).expand(batch_size, -1, -1)\n",
    "\n",
    "# Forward pass\n",
    "output, attn_weights = mha(x, x, x, mask=mask)\n",
    "\n",
    "print(f\"Input shape: {x.shape}\")\n",
    "print(f\"Output shape: {output.shape}\")\n",
    "print(\n",
    "    f\"Attention weights shape: {attn_weights.shape}\"\n",
    ")  # (batch, num_heads, seq_len, seq_len)\n",
    "print(f\"\\nNumber of parameters: {sum(p.numel() for p in mha.parameters()):,}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualizing Different Attention Heads\n",
    "\n",
    "Let's visualize what different heads are learning:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize first 4 heads\n",
    "fig, axes = plt.subplots(2, 2, figsize=(14, 12))\n",
    "axes = axes.flatten()\n",
    "\n",
    "for i in range(4):\n",
    "    attn = attn_weights[0, i].detach().cpu().numpy()  # First batch, i-th head\n",
    "\n",
    "    sns.heatmap(\n",
    "        attn,\n",
    "        ax=axes[i],\n",
    "        cmap=\"Blues\",\n",
    "        cbar=True,\n",
    "        cbar_kws={\"label\": \"Attention Weight\"},\n",
    "    )\n",
    "    axes[i].set_title(f\"Head {i + 1}\")\n",
    "    axes[i].set_xlabel(\"Key Position\")\n",
    "    axes[i].set_ylabel(\"Query Position\")\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"Notice how different heads learn different attention patterns!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 3: Positional Encoding\n",
    "\n",
    "### The Problem\n",
    "\n",
    "Attention is **permutation-invariant** - it doesn't know the order of tokens! We need to inject positional information.\n",
    "\n",
    "### Two Approaches:\n",
    "\n",
    "1. **Sinusoidal Positional Encoding** (original Transformer)\n",
    "2. **Rotary Positional Embeddings (RoPE)** (modern, used in our model)\n",
    "\n",
    "We'll focus on RoPE as it's more powerful and used in GPT-NeoX, LLaMA, and our Storyteller model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Rotary Positional Embeddings (RoPE)\n",
    "\n",
    "RoPE applies a rotation to query and key vectors based on their position. This elegantly encodes relative positions.\n",
    "\n",
    "Key advantages:\n",
    "- Encodes relative positions naturally\n",
    "- Works well for long sequences\n",
    "- No learnable parameters needed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RotaryPositionalEmbedding(nn.Module):\n",
    "    \"\"\"\n",
    "    Rotary Positional Embeddings (RoPE) from Su et al. (2021).\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, dim, max_seq_len=2048, base=10000):\n",
    "        super().__init__()\n",
    "        self.dim = dim\n",
    "        self.max_seq_len = max_seq_len\n",
    "        self.base = base\n",
    "\n",
    "        # Precompute frequencies\n",
    "        inv_freq = 1.0 / (base ** (torch.arange(0, dim, 2).float() / dim))\n",
    "        self.register_buffer(\"inv_freq\", inv_freq)\n",
    "\n",
    "        # Cache for efficiency\n",
    "        self._seq_len_cached = None\n",
    "        self._cos_cached = None\n",
    "        self._sin_cached = None\n",
    "\n",
    "    def _update_cache(self, seq_len, device):\n",
    "        \"\"\"Update cached cos/sin values if sequence length changed.\"\"\"\n",
    "        if seq_len != self._seq_len_cached:\n",
    "            self._seq_len_cached = seq_len\n",
    "            t = torch.arange(seq_len, device=device).type_as(self.inv_freq)\n",
    "            freqs = torch.outer(t, self.inv_freq)\n",
    "            emb = torch.cat((freqs, freqs), dim=-1)\n",
    "            self._cos_cached = emb.cos()[None, :, None, :]\n",
    "            self._sin_cached = emb.sin()[None, :, None, :]\n",
    "\n",
    "    def rotate_half(self, x):\n",
    "        \"\"\"Rotate half the hidden dims of the input.\"\"\"\n",
    "        x1, x2 = x[..., : x.shape[-1] // 2], x[..., x.shape[-1] // 2 :]\n",
    "        return torch.cat((-x2, x1), dim=-1)\n",
    "\n",
    "    def forward(self, q, k):\n",
    "        \"\"\"\n",
    "        Apply rotary embeddings to query and key tensors.\n",
    "\n",
    "        Args:\n",
    "            q: Query tensor (batch, seq_len, num_heads, d_k)\n",
    "            k: Key tensor (batch, seq_len, num_heads, d_k)\n",
    "        \"\"\"\n",
    "        seq_len = q.shape[1]\n",
    "        self._update_cache(seq_len, q.device)\n",
    "\n",
    "        # Apply rotation\n",
    "        q_embed = (q * self._cos_cached[:, :seq_len]) + (\n",
    "            self.rotate_half(q) * self._sin_cached[:, :seq_len]\n",
    "        )\n",
    "        k_embed = (k * self._cos_cached[:, :seq_len]) + (\n",
    "            self.rotate_half(k) * self._sin_cached[:, :seq_len]\n",
    "        )\n",
    "\n",
    "        return q_embed, k_embed\n",
    "\n",
    "\n",
    "# Test RoPE\n",
    "rope = RotaryPositionalEmbedding(dim=64)\n",
    "q = torch.randn(1, 10, 8, 64)  # (batch, seq_len, num_heads, d_k)\n",
    "k = torch.randn(1, 10, 8, 64)\n",
    "\n",
    "q_rot, k_rot = rope(q, k)\n",
    "print(f\"Query shape: {q_rot.shape}\")\n",
    "print(f\"Key shape: {k_rot.shape}\")\n",
    "print(\"Rotary embeddings applied successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 4: Feed-Forward Networks\n",
    "\n",
    "After attention, transformers apply a position-wise feed-forward network to each token independently.\n",
    "\n",
    "Structure:\n",
    "```\n",
    "FFN(x) = activation(x @ W1 + b1) @ W2 + b2\n",
    "```\n",
    "\n",
    "Typically:\n",
    "- First layer expands dimension (e.g., 512 → 2048)\n",
    "- Activation function (GELU, ReLU, etc.)\n",
    "- Second layer projects back (e.g., 2048 → 512)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FeedForward(nn.Module):\n",
    "    \"\"\"\n",
    "    Position-wise Feed-Forward Network.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, d_model, d_ff, dropout=0.1, activation=\"gelu\"):\n",
    "        super().__init__()\n",
    "        self.fc1 = nn.Linear(d_model, d_ff)\n",
    "        self.fc2 = nn.Linear(d_ff, d_model)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "        # Choose activation function\n",
    "        if activation == \"gelu\":\n",
    "            self.activation = nn.GELU()\n",
    "        elif activation == \"relu\":\n",
    "            self.activation = nn.ReLU()\n",
    "        else:\n",
    "            raise ValueError(f\"Unknown activation: {activation}\")\n",
    "\n",
    "    def forward(self, x):\n",
    "        # x: (batch, seq_len, d_model)\n",
    "        x = self.fc1(x)  # (batch, seq_len, d_ff)\n",
    "        x = self.activation(x)  # Apply non-linearity\n",
    "        x = self.dropout(x)\n",
    "        x = self.fc2(x)  # (batch, seq_len, d_model)\n",
    "        return x\n",
    "\n",
    "\n",
    "# Test\n",
    "ffn = FeedForward(d_model=512, d_ff=2048)\n",
    "x = torch.randn(2, 10, 512)\n",
    "output = ffn(x)\n",
    "print(f\"Input shape: {x.shape}\")\n",
    "print(f\"Output shape: {output.shape}\")\n",
    "print(f\"Parameters: {sum(p.numel() for p in ffn.parameters()):,}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 5: Building a Complete Transformer Block\n",
    "\n",
    "Now let's combine everything into a complete transformer decoder block:\n",
    "\n",
    "1. Multi-head self-attention (with causal mask)\n",
    "2. Add & Norm (residual connection + layer normalization)\n",
    "3. Feed-forward network\n",
    "4. Add & Norm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TransformerBlock(nn.Module):\n",
    "    \"\"\"\n",
    "    A single transformer decoder block.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, d_model, num_heads, d_ff, dropout=0.1):\n",
    "        super().__init__()\n",
    "\n",
    "        # Multi-head attention\n",
    "        self.attention = MultiHeadAttention(d_model, num_heads, dropout)\n",
    "\n",
    "        # Feed-forward network\n",
    "        self.ffn = FeedForward(d_model, d_ff, dropout)\n",
    "\n",
    "        # Layer normalization\n",
    "        self.ln1 = nn.LayerNorm(d_model)\n",
    "        self.ln2 = nn.LayerNorm(d_model)\n",
    "\n",
    "        # Dropout\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, x, mask=None):\n",
    "        \"\"\"\n",
    "        Forward pass through transformer block.\n",
    "\n",
    "        Args:\n",
    "            x: Input tensor (batch, seq_len, d_model)\n",
    "            mask: Optional causal mask\n",
    "        \"\"\"\n",
    "        # 1. Multi-head attention with residual connection\n",
    "        attn_output, attn_weights = self.attention(x, x, x, mask)\n",
    "        x = self.ln1(x + self.dropout(attn_output))  # Add & Norm\n",
    "\n",
    "        # 2. Feed-forward with residual connection\n",
    "        ffn_output = self.ffn(x)\n",
    "        x = self.ln2(x + self.dropout(ffn_output))  # Add & Norm\n",
    "\n",
    "        return x, attn_weights\n",
    "\n",
    "\n",
    "# Test a complete block\n",
    "block = TransformerBlock(d_model=512, num_heads=8, d_ff=2048)\n",
    "x = torch.randn(2, 10, 512)\n",
    "mask = create_causal_mask(10).unsqueeze(0).expand(2, -1, -1)\n",
    "\n",
    "output, attn = block(x, mask)\n",
    "print(f\"Input shape: {x.shape}\")\n",
    "print(f\"Output shape: {output.shape}\")\n",
    "print(f\"\\nTotal parameters: {sum(p.numel() for p in block.parameters()):,}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 6: Decoder-Only Architecture (GPT-Style)\n",
    "\n",
    "Our Storyteller model uses a **decoder-only** architecture, like GPT. Let's understand why:\n",
    "\n",
    "### Encoder-Decoder vs. Decoder-Only\n",
    "\n",
    "**Encoder-Decoder** (BERT, T5):\n",
    "- Encoder processes input without causal masking\n",
    "- Decoder generates output with cross-attention to encoder\n",
    "- Good for: translation, summarization (input → different output)\n",
    "\n",
    "**Decoder-Only** (GPT, LLaMA, our model):\n",
    "- Single stack of decoder blocks\n",
    "- Always uses causal masking\n",
    "- Good for: text generation, completion, few-shot learning\n",
    "\n",
    "### Why Decoder-Only for Story Generation?\n",
    "\n",
    "1. **Autoregressive generation**: Naturally suited for sequential text\n",
    "2. **Simplicity**: Fewer components to train\n",
    "3. **Flexibility**: Can handle various tasks via prompting\n",
    "4. **Scalability**: Easier to scale to billions of parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SimpleGPT(nn.Module):\n",
    "    \"\"\"\n",
    "    A simplified GPT-style decoder-only language model.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        vocab_size,\n",
    "        d_model=512,\n",
    "        num_layers=6,\n",
    "        num_heads=8,\n",
    "        d_ff=2048,\n",
    "        max_seq_len=1024,\n",
    "        dropout=0.1,\n",
    "    ):\n",
    "        super().__init__()\n",
    "\n",
    "        # Token embeddings\n",
    "        self.token_embedding = nn.Embedding(vocab_size, d_model)\n",
    "\n",
    "        # Positional embeddings (learnable)\n",
    "        self.pos_embedding = nn.Embedding(max_seq_len, d_model)\n",
    "\n",
    "        # Transformer blocks\n",
    "        self.blocks = nn.ModuleList(\n",
    "            [\n",
    "                TransformerBlock(d_model, num_heads, d_ff, dropout)\n",
    "                for _ in range(num_layers)\n",
    "            ]\n",
    "        )\n",
    "\n",
    "        # Final layer norm\n",
    "        self.ln_f = nn.LayerNorm(d_model)\n",
    "\n",
    "        # Output projection to vocabulary\n",
    "        self.lm_head = nn.Linear(d_model, vocab_size, bias=False)\n",
    "\n",
    "        # Dropout\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "        self.max_seq_len = max_seq_len\n",
    "\n",
    "    def forward(self, input_ids, labels=None):\n",
    "        \"\"\"\n",
    "        Forward pass.\n",
    "\n",
    "        Args:\n",
    "            input_ids: Token indices (batch, seq_len)\n",
    "            labels: Optional target tokens for loss computation\n",
    "        \"\"\"\n",
    "        batch_size, seq_len = input_ids.size()\n",
    "\n",
    "        # Create position indices\n",
    "        positions = torch.arange(0, seq_len, device=input_ids.device).unsqueeze(0)\n",
    "\n",
    "        # Embed tokens and positions\n",
    "        token_emb = self.token_embedding(input_ids)  # (batch, seq_len, d_model)\n",
    "        pos_emb = self.pos_embedding(positions)  # (1, seq_len, d_model)\n",
    "\n",
    "        # Combine embeddings\n",
    "        x = self.dropout(token_emb + pos_emb)\n",
    "\n",
    "        # Create causal mask\n",
    "        mask = create_causal_mask(seq_len).to(input_ids.device)\n",
    "        mask = mask.unsqueeze(0).expand(batch_size, -1, -1)\n",
    "\n",
    "        # Pass through transformer blocks\n",
    "        attention_maps = []\n",
    "        for block in self.blocks:\n",
    "            x, attn = block(x, mask)\n",
    "            attention_maps.append(attn)\n",
    "\n",
    "        # Final layer norm\n",
    "        x = self.ln_f(x)\n",
    "\n",
    "        # Project to vocabulary\n",
    "        logits = self.lm_head(x)  # (batch, seq_len, vocab_size)\n",
    "\n",
    "        # Compute loss if labels provided\n",
    "        loss = None\n",
    "        if labels is not None:\n",
    "            # Shift so predictions align with next token\n",
    "            shift_logits = logits[:, :-1, :].contiguous()\n",
    "            shift_labels = labels[:, 1:].contiguous()\n",
    "\n",
    "            # Flatten and compute cross-entropy\n",
    "            loss = F.cross_entropy(\n",
    "                shift_logits.view(-1, shift_logits.size(-1)), shift_labels.view(-1)\n",
    "            )\n",
    "\n",
    "        return {\"logits\": logits, \"loss\": loss, \"attention_maps\": attention_maps}\n",
    "\n",
    "\n",
    "# Create a small model\n",
    "model = SimpleGPT(\n",
    "    vocab_size=10000, d_model=256, num_layers=4, num_heads=4, d_ff=1024, max_seq_len=128\n",
    ")\n",
    "\n",
    "# Count parameters\n",
    "total_params = sum(p.numel() for p in model.parameters())\n",
    "print(f\"Model created with {total_params:,} parameters\")\n",
    "print(f\"Model size: ~{total_params * 4 / 1e6:.1f} MB (float32)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Testing the Model\n",
    "\n",
    "Let's do a forward pass with some dummy tokens:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create dummy input\n",
    "batch_size = 4\n",
    "seq_len = 20\n",
    "input_ids = torch.randint(0, 10000, (batch_size, seq_len))\n",
    "\n",
    "# Forward pass\n",
    "with torch.no_grad():\n",
    "    outputs = model(input_ids, labels=input_ids)\n",
    "\n",
    "print(f\"Input shape: {input_ids.shape}\")\n",
    "print(f\"Output logits shape: {outputs['logits'].shape}\")\n",
    "print(f\"Loss: {outputs['loss'].item():.4f}\")\n",
    "print(f\"Number of attention maps: {len(outputs['attention_maps'])}\")\n",
    "\n",
    "# Look at predictions for first token\n",
    "first_token_logits = outputs[\"logits\"][0, 0]  # First batch, first position\n",
    "top5_tokens = torch.topk(first_token_logits, k=5)\n",
    "print(f\"\\nTop 5 predicted token IDs for first position: {top5_tokens.indices.tolist()}\")\n",
    "print(f\"Their probabilities: {F.softmax(top5_tokens.values, dim=0).tolist()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary and Key Takeaways\n",
    "\n",
    "In this notebook, you learned:\n",
    "\n",
    "1. **Attention Mechanism**:\n",
    "   - Query, Key, Value paradigm\n",
    "   - Scaled dot-product attention\n",
    "   - Causal masking for autoregressive generation\n",
    "\n",
    "2. **Multi-Head Attention**:\n",
    "   - Multiple parallel attention \"heads\"\n",
    "   - Different heads learn different patterns\n",
    "   - Richer representation of relationships\n",
    "\n",
    "3. **Positional Encoding**:\n",
    "   - Why we need it (attention is permutation-invariant)\n",
    "   - Rotary embeddings (RoPE) for relative positions\n",
    "\n",
    "4. **Transformer Architecture**:\n",
    "   - Feed-forward networks\n",
    "   - Residual connections and layer normalization\n",
    "   - Complete transformer blocks\n",
    "\n",
    "5. **Decoder-Only Models**:\n",
    "   - GPT-style architecture\n",
    "   - Autoregressive text generation\n",
    "   - Why it works for story generation\n",
    "\n",
    "### What's Next?\n",
    "\n",
    "In the next notebook, we'll explore **Mixture of Experts (MoE)** - a technique to scale our model efficiently by using sparse expert routing.\n",
    "\n",
    "### Further Reading\n",
    "\n",
    "- [Attention Is All You Need](https://arxiv.org/abs/1706.03762) (Vaswani et al., 2017)\n",
    "- [RoFormer: Enhanced Transformer with Rotary Position Embedding](https://arxiv.org/abs/2104.09864) (Su et al., 2021)\n",
    "- [The Illustrated Transformer](http://jalammar.github.io/illustrated-transformer/) (Jay Alammar)\n",
    "- [GPT-3 Paper](https://arxiv.org/abs/2005.14165) (Brown et al., 2020)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise: Build Your Own Attention Variant\n",
    "\n",
    "Try implementing these variations:\n",
    "\n",
    "1. **Sliding Window Attention**: Limit attention to nearby tokens only\n",
    "2. **Cross-Attention**: Attention between two different sequences\n",
    "3. **Relative Position Bias**: Add learnable biases based on distance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code here!\n",
    "\n",
    "# Example: Sliding window attention\n",
    "def create_sliding_window_mask(seq_len, window_size):\n",
    "    \"\"\"\n",
    "    Create a mask that only allows attention within a sliding window.\n",
    "\n",
    "    Args:\n",
    "        seq_len: Sequence length\n",
    "        window_size: Size of attention window on each side\n",
    "    \"\"\"\n",
    "    # TODO: Implement this!\n",
    "    pass\n",
    "\n",
    "\n",
    "# Test your implementation\n",
    "# mask = create_sliding_window_mask(10, window_size=2)\n",
    "# plot_attention(mask.unsqueeze(0), tokens=[f\"T{i}\" for i in range(10)])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}