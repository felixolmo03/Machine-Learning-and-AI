{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Module 4: Training Basics\n",
    "\n",
    "In this notebook, we'll learn how to train a language model from scratch. We'll cover the essential components of a modern training pipeline.\n",
    "\n",
    "## Learning Objectives\n",
    "\n",
    "By the end of this notebook, you will:\n",
    "1. Understand the data pipeline for language model training\n",
    "2. Configure optimizers (AdamW) and learning rate schedules\n",
    "3. Implement mixed precision training for efficiency\n",
    "4. Build a basic training loop with gradient accumulation\n",
    "5. Monitor training metrics and diagnose issues\n",
    "6. Save and load checkpoints\n",
    "7. Train a small model on real data\n",
    "\n",
    "## What We'll Build\n",
    "\n",
    "A complete training pipeline including:\n",
    "- Custom dataset and dataloader\n",
    "- Training loop with validation\n",
    "- Learning rate scheduling (warmup + cosine decay)\n",
    "- Gradient clipping and accumulation\n",
    "- Checkpointing and resumption"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch.cuda.amp import autocast, GradScaler\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from pathlib import Path\n",
    "from tqdm.auto import tqdm\n",
    "import math\n",
    "\n",
    "# Set seeds\n",
    "torch.manual_seed(42)\n",
    "np.random.seed(42)\n",
    "\n",
    "# Configure plotting\n",
    "sns.set_style(\"whitegrid\")\n",
    "plt.rcParams[\"figure.figsize\"] = (12, 6)\n",
    "\n",
    "# Check device\n",
    "device = torch.device(\n",
    "    \"mps\"\n",
    "    if torch.backends.mps.is_available()\n",
    "    else \"cuda\"\n",
    "    if torch.cuda.is_available()\n",
    "    else \"cpu\"\n",
    ")\n",
    "print(f\"Using device: {device}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 1: Understanding the Data Pipeline\n",
    "\n",
    "### Language Model Training Data\n",
    "\n",
    "For autoregressive language models, we:\n",
    "1. Tokenize text into integer sequences\n",
    "2. Create sliding windows of length `max_seq_len`\n",
    "3. Use input[:-1] as input, input[1:] as target (next token prediction)\n",
    "\n",
    "Example:\n",
    "```\n",
    "Text: \"The cat sat on the mat\"\n",
    "Tokens: [1, 45, 23, 67, 12, 45, 89]\n",
    "Input:  [1, 45, 23, 67, 12, 45]  <- predict next token at each position\n",
    "Target: [45, 23, 67, 12, 45, 89] <- shifted by 1\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TextDataset(Dataset):\n",
    "    \"\"\"\n",
    "    Dataset for language model training.\n",
    "\n",
    "    Creates sliding windows over tokenized text.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        data_path: str,\n",
    "        max_seq_len: int = 256,\n",
    "        stride: int = None,\n",
    "    ):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            data_path: Path to tokenized data file (one token ID per line)\n",
    "            max_seq_len: Maximum sequence length\n",
    "            stride: Stride for sliding window (default: max_seq_len, no overlap)\n",
    "        \"\"\"\n",
    "        self.max_seq_len = max_seq_len\n",
    "        self.stride = stride or max_seq_len\n",
    "\n",
    "        # Load tokenized data\n",
    "        print(f\"Loading data from {data_path}...\")\n",
    "        with open(data_path, \"r\") as f:\n",
    "            # Assume data is space-separated token IDs\n",
    "            self.tokens = [int(x) for line in f for x in line.strip().split()]\n",
    "\n",
    "        print(f\"Loaded {len(self.tokens):,} tokens\")\n",
    "\n",
    "        # Calculate number of sequences\n",
    "        self.num_sequences = max(1, (len(self.tokens) - max_seq_len) // self.stride + 1)\n",
    "        print(f\"Created {self.num_sequences:,} sequences of length {max_seq_len}\")\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.num_sequences\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        \"\"\"\n",
    "        Get a single training example.\n",
    "\n",
    "        Returns:\n",
    "            dict with 'input_ids', 'attention_mask', and 'labels'\n",
    "        \"\"\"\n",
    "        # Get starting position\n",
    "        start_idx = idx * self.stride\n",
    "        end_idx = start_idx + self.max_seq_len\n",
    "\n",
    "        # Extract sequence\n",
    "        sequence = self.tokens[start_idx:end_idx]\n",
    "\n",
    "        # Pad if necessary (for last sequence)\n",
    "        if len(sequence) < self.max_seq_len:\n",
    "            sequence = sequence + [0] * (self.max_seq_len - len(sequence))\n",
    "\n",
    "        # Convert to tensor\n",
    "        input_ids = torch.tensor(sequence, dtype=torch.long)\n",
    "\n",
    "        # Create attention mask (1 for real tokens, 0 for padding)\n",
    "        attention_mask = (input_ids != 0).long()\n",
    "\n",
    "        # Labels are same as input (we'll shift inside the model)\n",
    "        labels = input_ids.clone()\n",
    "\n",
    "        return {\n",
    "            \"input_ids\": input_ids,\n",
    "            \"attention_mask\": attention_mask,\n",
    "            \"labels\": labels,\n",
    "        }\n",
    "\n",
    "\n",
    "# Create a dummy dataset for demonstration\n",
    "# In practice, you'd use real tokenized data\n",
    "dummy_data_path = Path(\"dummy_tokens.txt\")\n",
    "with open(dummy_data_path, \"w\") as f:\n",
    "    # Write random token IDs\n",
    "    tokens = np.random.randint(1, 1000, size=10000)\n",
    "    f.write(\" \".join(map(str, tokens)))\n",
    "\n",
    "# Create dataset\n",
    "dataset = TextDataset(str(dummy_data_path), max_seq_len=128)\n",
    "\n",
    "# Test it\n",
    "sample = dataset[0]\n",
    "print(\"\\nSample batch:\")\n",
    "print(f\"  input_ids shape: {sample['input_ids'].shape}\")\n",
    "print(f\"  attention_mask shape: {sample['attention_mask'].shape}\")\n",
    "print(f\"  labels shape: {sample['labels'].shape}\")\n",
    "print(f\"  First 10 tokens: {sample['input_ids'][:10].tolist()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating DataLoaders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split into train/val\n",
    "train_size = int(0.9 * len(dataset))\n",
    "val_size = len(dataset) - train_size\n",
    "train_dataset, val_dataset = torch.utils.data.random_split(\n",
    "    dataset, [train_size, val_size]\n",
    ")\n",
    "\n",
    "# Create dataloaders\n",
    "batch_size = 8\n",
    "train_loader = DataLoader(\n",
    "    train_dataset,\n",
    "    batch_size=batch_size,\n",
    "    shuffle=True,\n",
    "    num_workers=2,\n",
    "    pin_memory=True,\n",
    ")\n",
    "\n",
    "val_loader = DataLoader(\n",
    "    val_dataset,\n",
    "    batch_size=batch_size,\n",
    "    shuffle=False,\n",
    "    num_workers=2,\n",
    "    pin_memory=True,\n",
    ")\n",
    "\n",
    "print(f\"Training batches: {len(train_loader)}\")\n",
    "print(f\"Validation batches: {len(val_loader)}\")\n",
    "print(f\"Effective batch size: {batch_size}\")\n",
    "\n",
    "# Test loading a batch\n",
    "batch = next(iter(train_loader))\n",
    "print(\"\\nBatch shapes:\")\n",
    "for key, val in batch.items():\n",
    "    print(f\"  {key}: {val.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 2: Building a Simple Model\n",
    "\n",
    "For this tutorial, we'll use a small GPT-style model (from notebook 02)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Simplified model for demonstration\n",
    "class TinyGPT(nn.Module):\n",
    "    \"\"\"\n",
    "    A tiny GPT-style model for training demonstration.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        vocab_size: int = 10000,\n",
    "        d_model: int = 256,\n",
    "        num_layers: int = 4,\n",
    "        num_heads: int = 4,\n",
    "        d_ff: int = 1024,\n",
    "        max_seq_len: int = 256,\n",
    "        dropout: float = 0.1,\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.d_model = d_model\n",
    "\n",
    "        # Embeddings\n",
    "        self.token_embedding = nn.Embedding(vocab_size, d_model)\n",
    "        self.pos_embedding = nn.Embedding(max_seq_len, d_model)\n",
    "\n",
    "        # Transformer blocks (simplified)\n",
    "        self.blocks = nn.ModuleList(\n",
    "            [\n",
    "                nn.TransformerEncoderLayer(\n",
    "                    d_model=d_model,\n",
    "                    nhead=num_heads,\n",
    "                    dim_feedforward=d_ff,\n",
    "                    dropout=dropout,\n",
    "                    batch_first=True,\n",
    "                )\n",
    "                for _ in range(num_layers)\n",
    "            ]\n",
    "        )\n",
    "\n",
    "        # Output\n",
    "        self.ln_f = nn.LayerNorm(d_model)\n",
    "        self.lm_head = nn.Linear(d_model, vocab_size, bias=False)\n",
    "\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "        # Initialize weights\n",
    "        self.apply(self._init_weights)\n",
    "\n",
    "    def _init_weights(self, module):\n",
    "        \"\"\"Initialize model weights.\"\"\"\n",
    "        if isinstance(module, nn.Linear):\n",
    "            torch.nn.init.normal_(module.weight, mean=0.0, std=0.02)\n",
    "            if module.bias is not None:\n",
    "                torch.nn.init.zeros_(module.bias)\n",
    "        elif isinstance(module, nn.Embedding):\n",
    "            torch.nn.init.normal_(module.weight, mean=0.0, std=0.02)\n",
    "\n",
    "    def forward(self, input_ids, attention_mask=None, labels=None):\n",
    "        \"\"\"\n",
    "        Forward pass.\n",
    "\n",
    "        Args:\n",
    "            input_ids: Token indices (batch, seq_len)\n",
    "            attention_mask: Attention mask (batch, seq_len)\n",
    "            labels: Target tokens for loss computation (batch, seq_len)\n",
    "        \"\"\"\n",
    "        batch_size, seq_len = input_ids.size()\n",
    "\n",
    "        # Create position IDs\n",
    "        positions = torch.arange(0, seq_len, device=input_ids.device).unsqueeze(0)\n",
    "\n",
    "        # Embed\n",
    "        token_emb = self.token_embedding(input_ids)\n",
    "        pos_emb = self.pos_embedding(positions)\n",
    "        x = self.dropout(token_emb + pos_emb)\n",
    "\n",
    "        # Create causal mask\n",
    "        causal_mask = torch.triu(\n",
    "            torch.ones(seq_len, seq_len, device=input_ids.device) * float(\"-inf\"),\n",
    "            diagonal=1,\n",
    "        )\n",
    "\n",
    "        # Transformer blocks\n",
    "        for block in self.blocks:\n",
    "            x = block(x, src_mask=causal_mask)\n",
    "\n",
    "        # Output\n",
    "        x = self.ln_f(x)\n",
    "        logits = self.lm_head(x)\n",
    "\n",
    "        # Compute loss if labels provided\n",
    "        loss = None\n",
    "        if labels is not None:\n",
    "            # Shift logits and labels for next token prediction\n",
    "            shift_logits = logits[:, :-1, :].contiguous()\n",
    "            shift_labels = labels[:, 1:].contiguous()\n",
    "\n",
    "            # Flatten and compute cross entropy\n",
    "            loss = F.cross_entropy(\n",
    "                shift_logits.view(-1, shift_logits.size(-1)),\n",
    "                shift_labels.view(-1),\n",
    "                ignore_index=0,  # Ignore padding\n",
    "            )\n",
    "\n",
    "        return {\"logits\": logits, \"loss\": loss}\n",
    "\n",
    "\n",
    "# Create model\n",
    "model = TinyGPT(\n",
    "    vocab_size=1000,\n",
    "    d_model=256,\n",
    "    num_layers=4,\n",
    "    num_heads=4,\n",
    "    d_ff=1024,\n",
    "    max_seq_len=128,\n",
    ").to(device)\n",
    "\n",
    "total_params = sum(p.numel() for p in model.parameters())\n",
    "trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "\n",
    "print(\"Model created!\")\n",
    "print(f\"  Total parameters: {total_params:,}\")\n",
    "print(f\"  Trainable parameters: {trainable_params:,}\")\n",
    "print(f\"  Model size: ~{total_params * 4 / 1e6:.1f} MB (float32)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 3: Optimizer Configuration\n",
    "\n",
    "### AdamW: The Standard Choice\n",
    "\n",
    "**AdamW** (Adam with decoupled weight decay) is the most common optimizer for transformers.\n",
    "\n",
    "Key hyperparameters:\n",
    "- **Learning rate**: 1e-4 to 3e-4 typical\n",
    "- **Weight decay**: 0.01 to 0.1 (for regularization)\n",
    "- **Betas**: (0.9, 0.95) or (0.9, 0.999)\n",
    "- **Epsilon**: 1e-8\n",
    "\n",
    "### Weight Decay Groups\n",
    "\n",
    "We typically **don't** apply weight decay to:\n",
    "- Layer norm parameters\n",
    "- Biases\n",
    "- Embeddings (sometimes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def configure_optimizer(model, learning_rate=3e-4, weight_decay=0.1, betas=(0.9, 0.95)):\n",
    "    \"\"\"\n",
    "    Configure AdamW optimizer with proper weight decay groups.\n",
    "    \"\"\"\n",
    "    # Separate parameters into weight decay and no weight decay groups\n",
    "    decay = set()\n",
    "    no_decay = set()\n",
    "\n",
    "    for name, param in model.named_parameters():\n",
    "        if not param.requires_grad:\n",
    "            continue\n",
    "\n",
    "        # No weight decay for layer norms and biases\n",
    "        if \"ln\" in name or \"bias\" in name or \"norm\" in name:\n",
    "            no_decay.add(param)\n",
    "        else:\n",
    "            decay.add(param)\n",
    "\n",
    "    # Create parameter groups\n",
    "    param_groups = [\n",
    "        {\"params\": list(decay), \"weight_decay\": weight_decay},\n",
    "        {\"params\": list(no_decay), \"weight_decay\": 0.0},\n",
    "    ]\n",
    "\n",
    "    print(\"Optimizer groups:\")\n",
    "    print(f\"  With weight decay: {len(decay)} params\")\n",
    "    print(f\"  Without weight decay: {len(no_decay)} params\")\n",
    "\n",
    "    # Create optimizer\n",
    "    optimizer = torch.optim.AdamW(\n",
    "        param_groups,\n",
    "        lr=learning_rate,\n",
    "        betas=betas,\n",
    "        eps=1e-8,\n",
    "    )\n",
    "\n",
    "    return optimizer\n",
    "\n",
    "\n",
    "optimizer = configure_optimizer(model, learning_rate=3e-4, weight_decay=0.1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 4: Learning Rate Scheduling\n",
    "\n",
    "### Cosine Schedule with Warmup\n",
    "\n",
    "Best practice for transformer training:\n",
    "\n",
    "1. **Warmup** (first 5-10% of training):\n",
    "   - Linearly increase LR from 0 to max_lr\n",
    "   - Stabilizes training at the start\n",
    "\n",
    "2. **Cosine Decay** (remaining training):\n",
    "   - Smoothly decrease LR following cosine curve\n",
    "   - Often to 0.1 Ã— max_lr (not zero)\n",
    "\n",
    "$$\\text{lr}(t) = \\text{lr}_{\\text{min}} + \\frac{1}{2}(\\text{lr}_{\\text{max}} - \\text{lr}_{\\text{min}})\\left(1 + \\cos\\left(\\frac{t - t_{\\text{warmup}}}{t_{\\text{max}} - t_{\\text{warmup}}}\\pi\\right)\\right)$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_cosine_schedule_with_warmup(\n",
    "    optimizer,\n",
    "    num_warmup_steps: int,\n",
    "    num_training_steps: int,\n",
    "    min_lr_ratio: float = 0.1,\n",
    "):\n",
    "    \"\"\"\n",
    "    Create a learning rate scheduler with linear warmup and cosine decay.\n",
    "\n",
    "    Args:\n",
    "        optimizer: The optimizer\n",
    "        num_warmup_steps: Number of warmup steps\n",
    "        num_training_steps: Total number of training steps\n",
    "        min_lr_ratio: Minimum LR as a ratio of max LR\n",
    "    \"\"\"\n",
    "\n",
    "    def lr_lambda(current_step):\n",
    "        # Warmup\n",
    "        if current_step < num_warmup_steps:\n",
    "            return float(current_step) / float(max(1, num_warmup_steps))\n",
    "\n",
    "        # Cosine decay\n",
    "        progress = float(current_step - num_warmup_steps) / float(\n",
    "            max(1, num_training_steps - num_warmup_steps)\n",
    "        )\n",
    "        return min_lr_ratio + (1 - min_lr_ratio) * 0.5 * (\n",
    "            1.0 + math.cos(math.pi * progress)\n",
    "        )\n",
    "\n",
    "    return torch.optim.lr_scheduler.LambdaLR(optimizer, lr_lambda)\n",
    "\n",
    "\n",
    "# Create scheduler\n",
    "num_epochs = 3\n",
    "num_training_steps = len(train_loader) * num_epochs\n",
    "num_warmup_steps = int(0.1 * num_training_steps)  # 10% warmup\n",
    "\n",
    "scheduler = get_cosine_schedule_with_warmup(\n",
    "    optimizer,\n",
    "    num_warmup_steps=num_warmup_steps,\n",
    "    num_training_steps=num_training_steps,\n",
    ")\n",
    "\n",
    "# Visualize the schedule\n",
    "lrs = []\n",
    "for step in range(num_training_steps):\n",
    "    lrs.append(scheduler.get_last_lr()[0])\n",
    "    scheduler.step()\n",
    "\n",
    "# Reset scheduler\n",
    "scheduler = get_cosine_schedule_with_warmup(\n",
    "    optimizer, num_warmup_steps, num_training_steps\n",
    ")\n",
    "\n",
    "# Plot\n",
    "plt.figure(figsize=(12, 5))\n",
    "plt.plot(lrs, linewidth=2)\n",
    "plt.axvline(\n",
    "    x=num_warmup_steps,\n",
    "    color=\"red\",\n",
    "    linestyle=\"--\",\n",
    "    label=f\"End of warmup (step {num_warmup_steps})\",\n",
    ")\n",
    "plt.xlabel(\"Training Step\")\n",
    "plt.ylabel(\"Learning Rate\")\n",
    "plt.title(\"Learning Rate Schedule: Warmup + Cosine Decay\")\n",
    "plt.legend()\n",
    "plt.grid(alpha=0.3)\n",
    "plt.show()\n",
    "\n",
    "print(f\"Total training steps: {num_training_steps}\")\n",
    "print(f\"Warmup steps: {num_warmup_steps}\")\n",
    "print(f\"Max LR: {max(lrs):.2e}\")\n",
    "print(f\"Min LR: {min(lrs):.2e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 5: Mixed Precision Training\n",
    "\n",
    "### Why Mixed Precision?\n",
    "\n",
    "- **Faster**: FP16/BF16 operations are 2-3x faster\n",
    "- **Less memory**: Half the memory per parameter\n",
    "- **Same accuracy**: With proper loss scaling\n",
    "\n",
    "### FP16 vs BF16:\n",
    "\n",
    "- **FP16**: Needs loss scaling, more compatible\n",
    "- **BF16**: Better for training, no loss scaling needed, requires newer GPUs\n",
    "\n",
    "We'll use PyTorch's `autocast` and `GradScaler`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check if we can use mixed precision\n",
    "use_amp = device.type in [\"cuda\", \"mps\"]\n",
    "dtype = torch.bfloat16 if torch.cuda.is_bf16_supported() else torch.float16\n",
    "\n",
    "# Create gradient scaler (only for FP16)\n",
    "scaler = GradScaler() if (use_amp and dtype == torch.float16) else None\n",
    "\n",
    "print(f\"Mixed precision training: {use_amp}\")\n",
    "if use_amp:\n",
    "    print(f\"  Using dtype: {dtype}\")\n",
    "    print(f\"  Using GradScaler: {scaler is not None}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 6: Training Loop\n",
    "\n",
    "Now let's implement a complete training loop with:\n",
    "- Mixed precision\n",
    "- Gradient accumulation\n",
    "- Gradient clipping\n",
    "- Progress tracking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_epoch(\n",
    "    model,\n",
    "    train_loader,\n",
    "    optimizer,\n",
    "    scheduler,\n",
    "    device,\n",
    "    scaler=None,\n",
    "    gradient_accumulation_steps=1,\n",
    "    max_grad_norm=1.0,\n",
    "):\n",
    "    \"\"\"\n",
    "    Train for one epoch.\n",
    "    \"\"\"\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "\n",
    "    pbar = tqdm(train_loader, desc=\"Training\")\n",
    "\n",
    "    for step, batch in enumerate(pbar):\n",
    "        # Move to device\n",
    "        input_ids = batch[\"input_ids\"].to(device)\n",
    "        attention_mask = batch[\"attention_mask\"].to(device)\n",
    "        labels = batch[\"labels\"].to(device)\n",
    "\n",
    "        # Forward pass with mixed precision\n",
    "        with autocast(device_type=device.type, dtype=dtype, enabled=use_amp):\n",
    "            outputs = model(\n",
    "                input_ids=input_ids,\n",
    "                attention_mask=attention_mask,\n",
    "                labels=labels,\n",
    "            )\n",
    "            loss = outputs[\"loss\"] / gradient_accumulation_steps\n",
    "\n",
    "        # Backward pass\n",
    "        if scaler is not None:\n",
    "            scaler.scale(loss).backward()\n",
    "        else:\n",
    "            loss.backward()\n",
    "\n",
    "        # Gradient accumulation\n",
    "        if (step + 1) % gradient_accumulation_steps == 0:\n",
    "            # Gradient clipping\n",
    "            if scaler is not None:\n",
    "                scaler.unscale_(optimizer)\n",
    "\n",
    "            torch.nn.utils.clip_grad_norm_(model.parameters(), max_grad_norm)\n",
    "\n",
    "            # Optimizer step\n",
    "            if scaler is not None:\n",
    "                scaler.step(optimizer)\n",
    "                scaler.update()\n",
    "            else:\n",
    "                optimizer.step()\n",
    "\n",
    "            scheduler.step()\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "        # Track metrics\n",
    "        total_loss += loss.item() * gradient_accumulation_steps\n",
    "\n",
    "        # Update progress bar\n",
    "        pbar.set_postfix(\n",
    "            {\n",
    "                \"loss\": f\"{loss.item() * gradient_accumulation_steps:.4f}\",\n",
    "                \"lr\": f\"{scheduler.get_last_lr()[0]:.2e}\",\n",
    "            }\n",
    "        )\n",
    "\n",
    "    return total_loss / len(train_loader)\n",
    "\n",
    "\n",
    "@torch.no_grad()\n",
    "def evaluate(model, val_loader, device):\n",
    "    \"\"\"\n",
    "    Evaluate on validation set.\n",
    "    \"\"\"\n",
    "    model.eval()\n",
    "    total_loss = 0\n",
    "\n",
    "    for batch in tqdm(val_loader, desc=\"Evaluating\"):\n",
    "        input_ids = batch[\"input_ids\"].to(device)\n",
    "        attention_mask = batch[\"attention_mask\"].to(device)\n",
    "        labels = batch[\"labels\"].to(device)\n",
    "\n",
    "        with autocast(device_type=device.type, dtype=dtype, enabled=use_amp):\n",
    "            outputs = model(\n",
    "                input_ids=input_ids,\n",
    "                attention_mask=attention_mask,\n",
    "                labels=labels,\n",
    "            )\n",
    "\n",
    "        total_loss += outputs[\"loss\"].item()\n",
    "\n",
    "    avg_loss = total_loss / len(val_loader)\n",
    "    perplexity = math.exp(avg_loss)\n",
    "\n",
    "    return {\"loss\": avg_loss, \"perplexity\": perplexity}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 7: Run Training!\n",
    "\n",
    "Let's train our model for a few epochs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training configuration\n",
    "num_epochs = 3\n",
    "gradient_accumulation_steps = 4\n",
    "max_grad_norm = 1.0\n",
    "\n",
    "# Track metrics\n",
    "train_losses = []\n",
    "val_losses = []\n",
    "val_perplexities = []\n",
    "\n",
    "print(f\"Starting training for {num_epochs} epochs...\")\n",
    "print(f\"Effective batch size: {batch_size * gradient_accumulation_steps}\")\n",
    "print()\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    print(f\"Epoch {epoch + 1}/{num_epochs}\")\n",
    "\n",
    "    # Train\n",
    "    train_loss = train_epoch(\n",
    "        model,\n",
    "        train_loader,\n",
    "        optimizer,\n",
    "        scheduler,\n",
    "        device,\n",
    "        scaler,\n",
    "        gradient_accumulation_steps,\n",
    "        max_grad_norm,\n",
    "    )\n",
    "    train_losses.append(train_loss)\n",
    "\n",
    "    # Evaluate\n",
    "    val_metrics = evaluate(model, val_loader, device)\n",
    "    val_losses.append(val_metrics[\"loss\"])\n",
    "    val_perplexities.append(val_metrics[\"perplexity\"])\n",
    "\n",
    "    print(f\"  Train Loss: {train_loss:.4f}\")\n",
    "    print(f\"  Val Loss: {val_metrics['loss']:.4f}\")\n",
    "    print(f\"  Val Perplexity: {val_metrics['perplexity']:.2f}\")\n",
    "    print()\n",
    "\n",
    "print(\"Training complete!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualize Training Progress"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "# Plot losses\n",
    "epochs = range(1, num_epochs + 1)\n",
    "ax1.plot(epochs, train_losses, \"o-\", label=\"Train Loss\", linewidth=2)\n",
    "ax1.plot(epochs, val_losses, \"s-\", label=\"Val Loss\", linewidth=2)\n",
    "ax1.set_xlabel(\"Epoch\")\n",
    "ax1.set_ylabel(\"Loss\")\n",
    "ax1.set_title(\"Training and Validation Loss\")\n",
    "ax1.legend()\n",
    "ax1.grid(alpha=0.3)\n",
    "\n",
    "# Plot perplexity\n",
    "ax2.plot(epochs, val_perplexities, \"o-\", color=\"green\", linewidth=2)\n",
    "ax2.set_xlabel(\"Epoch\")\n",
    "ax2.set_ylabel(\"Perplexity\")\n",
    "ax2.set_title(\"Validation Perplexity\")\n",
    "ax2.grid(alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(f\"Final validation perplexity: {val_perplexities[-1]:.2f}\")\n",
    "print(\"Lower is better! (Perfect model = 1.0, random model = vocab_size)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 8: Checkpointing\n",
    "\n",
    "Always save checkpoints during training!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_checkpoint(model, optimizer, scheduler, epoch, loss, path):\n",
    "    \"\"\"\n",
    "    Save a training checkpoint.\n",
    "    \"\"\"\n",
    "    checkpoint = {\n",
    "        \"epoch\": epoch,\n",
    "        \"model_state_dict\": model.state_dict(),\n",
    "        \"optimizer_state_dict\": optimizer.state_dict(),\n",
    "        \"scheduler_state_dict\": scheduler.state_dict(),\n",
    "        \"loss\": loss,\n",
    "    }\n",
    "\n",
    "    torch.save(checkpoint, path)\n",
    "    print(f\"Checkpoint saved to {path}\")\n",
    "\n",
    "\n",
    "def load_checkpoint(model, optimizer, scheduler, path):\n",
    "    \"\"\"\n",
    "    Load a training checkpoint.\n",
    "    \"\"\"\n",
    "    checkpoint = torch.load(path, map_location=device)\n",
    "\n",
    "    model.load_state_dict(checkpoint[\"model_state_dict\"])\n",
    "    optimizer.load_state_dict(checkpoint[\"optimizer_state_dict\"])\n",
    "    scheduler.load_state_dict(checkpoint[\"scheduler_state_dict\"])\n",
    "\n",
    "    print(f\"Checkpoint loaded from {path}\")\n",
    "    print(f\"  Resuming from epoch {checkpoint['epoch']}\")\n",
    "    print(f\"  Previous loss: {checkpoint['loss']:.4f}\")\n",
    "\n",
    "    return checkpoint[\"epoch\"]\n",
    "\n",
    "\n",
    "# Save a checkpoint\n",
    "checkpoint_dir = Path(\"checkpoints\")\n",
    "checkpoint_dir.mkdir(exist_ok=True)\n",
    "\n",
    "save_checkpoint(\n",
    "    model,\n",
    "    optimizer,\n",
    "    scheduler,\n",
    "    epoch=num_epochs,\n",
    "    loss=val_losses[-1],\n",
    "    path=checkpoint_dir / \"model_final.pt\",\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary and Key Takeaways\n",
    "\n",
    "In this notebook, you learned:\n",
    "\n",
    "1. **Data Pipeline**:\n",
    "   - Creating sliding windows over text\n",
    "   - Proper batching and data loading\n",
    "   - Input/target pairs for next token prediction\n",
    "\n",
    "2. **Optimizer Setup**:\n",
    "   - AdamW with weight decay\n",
    "   - Separating parameters into decay groups\n",
    "   - Typical hyperparameters\n",
    "\n",
    "3. **Learning Rate Scheduling**:\n",
    "   - Warmup for stability\n",
    "   - Cosine decay for smooth convergence\n",
    "   - Typical warmup ratios (5-10%)\n",
    "\n",
    "4. **Mixed Precision Training**:\n",
    "   - FP16/BF16 for efficiency\n",
    "   - Using autocast and GradScaler\n",
    "   - 2-3x speedup with same accuracy\n",
    "\n",
    "5. **Training Loop**:\n",
    "   - Gradient accumulation for larger effective batch sizes\n",
    "   - Gradient clipping for stability\n",
    "   - Progress tracking and validation\n",
    "\n",
    "6. **Checkpointing**:\n",
    "   - Saving model state\n",
    "   - Resuming training\n",
    "\n",
    "### Key Metrics:\n",
    "\n",
    "- **Loss**: Cross-entropy loss (lower is better)\n",
    "- **Perplexity**: exp(loss), interpretable as \"branching factor\" (lower is better)\n",
    "- **Learning Rate**: Monitor to ensure proper scheduling\n",
    "\n",
    "### What's Next?\n",
    "\n",
    "In the final notebook, we'll:\n",
    "- Train a complete MoE model on real data\n",
    "- Integrate MLflow for experiment tracking\n",
    "- Implement text generation (inference)\n",
    "- Generate stories with our trained model!\n",
    "\n",
    "### Further Reading\n",
    "\n",
    "- [Mixed Precision Training](https://arxiv.org/abs/1710.03740) (Micikevicius et al., 2017)\n",
    "- [Decoupled Weight Decay Regularization](https://arxiv.org/abs/1711.05101) (Loshchilov & Hutter, 2017)\n",
    "- [Accurate, Large Minibatch SGD](https://arxiv.org/abs/1706.02677) (Goyal et al., 2017)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise: Tune Hyperparameters\n",
    "\n",
    "Try different hyperparameters and observe the effects:\n",
    "\n",
    "1. **Learning rate**: Try 1e-4, 3e-4, 1e-3\n",
    "2. **Warmup ratio**: Try 0.05, 0.1, 0.2\n",
    "3. **Weight decay**: Try 0, 0.01, 0.1\n",
    "4. **Gradient accumulation**: Try 1, 2, 4, 8\n",
    "\n",
    "Questions to explore:\n",
    "- How does learning rate affect convergence speed?\n",
    "- What happens without warmup?\n",
    "- How does weight decay affect generalization (train vs val loss)?\n",
    "- What's the effect of larger effective batch sizes?"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
