{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5"
   },
   "outputs": [],
   "source": [
    "# This Python 3 environment comes with many helpful analytics libraries installed\n",
    "# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n",
    "# For example, here's several helpful packages to load\n",
    "\n",
    "\n",
    "# Input data files are available in the read-only \"../input/\" directory\n",
    "# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n",
    "\n",
    "import os\n",
    "\n",
    "for dirname, _, filenames in os.walk(\"./\"):\n",
    "    for filename in filenames:\n",
    "        print(os.path.join(dirname, filename))\n",
    "\n",
    "# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\"\n",
    "# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Frame the problem\n",
    "Using the customer description, Define the problem your trying to solve in your own words (remember this is not technial but must be specific so the customer understands the project"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We are tasked with creating an automated tetris player that can successfully play for an extended period of time. By using a Reinforcement Learning model we will achieve this goal with the several characteristics we can define and configure within the model to make it properly play the game."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Get the Data \n",
    "Define how you recieved the data (provided, gathered..)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The data is collected by real-time training through trial and error, the model observes the current state takes and action, then recieves either positive or negative feedback. Data is collected by completing several games overtime. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Explore the Data\n",
    "Gain insights into the data you have from step 2, making sure to identify any bias"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The board consists of a 2D grid that is 10 x 20 that can take in binary values (0 = empty, 1 = filled). There are 7 types of blocks that will be taken into account along with the current roation state of those shapes. Some of the features that will be derived is the board height before each piece is placed, number of holes, bumpiness, number of complete lines, and the individual column heights. The model will be able to rotate the piece 0, 80, 180, or 270 degrees and move its horizontal position as long as it's within the confinments of the board. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4.Prepare the Data\n",
    "\n",
    "\n",
    "Apply any data transformations and explain what and why\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To properly prepare the data we will need to flatten the 10x20 board to a 200-dimensional vector and keep the normslization of 0 for empty 1 for filled. Then feature engineering will include the hand-extracted features. We then need to scale the rewards to a resonable range where positive awards outweigh the negative awards to fuel efficiency in a Reinforcement Learning model. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5. Model the data\n",
    "Using selected ML models, experment with your choices and describe your findings. Finish by selecting a Model to continue with\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I first attempted creating a DQN model however it came to my attention that not only was each generation very slow but it kept overfitting and learning mostly from failures and little to no successes so the results were very sparse and it never properly learned. I ended up using a genetic algorithm with heuristic weights which worked much better due to the immediate feedback and the heuristic weights were much more comparable to the factors that play into Tetris those being height, holes, bumpiness, etc... after each generation the model improves even slightly and trains much faster and efficiently.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 6. Fine Tune the Model\n",
    "\n",
    "With the select model descibe the steps taken to acheve the best rusults possiable \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Evolutionary Algorithm for Tetris\n",
    "Evolves optimal weights for long-running games\n",
    "Trains headless, runs with visual\n",
    "\"\"\"\n",
    "from copy import deepcopy\n",
    "import numpy as np\n",
    "import random\n",
    "import os\n",
    "import pickle\n",
    "\n",
    "# Configuration\n",
    "TRAINING_MODE = True  # False means run the best model saved\n",
    "CHECKPOINT_DIR = \"checkpoints\"\n",
    "os.makedirs(CHECKPOINT_DIR, exist_ok=True)\n",
    "CHECKPOINT_PATH = os.path.join(CHECKPOINT_DIR, \"evolution.pkl\")\n",
    "LOG_FILE = os.path.join(CHECKPOINT_DIR, \"evolution_log.txt\")\n",
    "\n",
    "POPULATION_SIZE = 10  \n",
    "ELITE_SIZE = 4  \n",
    "MUTATION_RATE = 0.4 \n",
    "MUTATION_STRENGTH = 0.25  \n",
    "GAMES_PER_INDIVIDUAL = 5  \n",
    "\n",
    "\n",
    "def evaluate_board(board_array):\n",
    "    \"\"\"Calculate board features\"\"\"\n",
    "    height = len(board_array)\n",
    "    width = len(board_array[0])\n",
    "    \n",
    "    heights = []\n",
    "    for col in range(width):\n",
    "        col_height = 0\n",
    "        for row in range(height):\n",
    "            if board_array[row][col]:\n",
    "                col_height = row + 1\n",
    "        heights.append(col_height)\n",
    "    \n",
    "    aggregate_height = sum(heights)\n",
    "    max_height = max(heights) if heights else 0\n",
    "    \n",
    "    holes = 0\n",
    "    for col in range(width):\n",
    "        found_block = False\n",
    "        for row in range(height - 1, -1, -1):\n",
    "            if board_array[row][col]:\n",
    "                found_block = True\n",
    "            elif found_block:\n",
    "                holes += 1\n",
    "    \n",
    "    bumpiness = sum(abs(heights[i] - heights[i + 1]) for i in range(len(heights) - 1))\n",
    "    complete_lines = sum(1 for row in board_array if all(row))\n",
    "    \n",
    "    wells = 0\n",
    "    for i in range(width):\n",
    "        if i == 0:\n",
    "            if heights[i] < heights[i + 1]:\n",
    "                wells += heights[i + 1] - heights[i]\n",
    "        elif i == width - 1:\n",
    "            if heights[i] < heights[i - 1]:\n",
    "                wells += heights[i - 1] - heights[i]\n",
    "        else:\n",
    "            if heights[i] < heights[i - 1] and heights[i] < heights[i + 1]:\n",
    "                wells += min(heights[i - 1], heights[i + 1]) - heights[i]\n",
    "    \n",
    "    return {\n",
    "        'aggregate_height': aggregate_height,\n",
    "        'max_height': max_height,\n",
    "        'holes': holes,\n",
    "        'bumpiness': bumpiness,\n",
    "        'complete_lines': complete_lines,\n",
    "        'wells': wells\n",
    "    }\n",
    "\n",
    "def simulate_placement(board, piece, x):\n",
    "    \"\"\"Simulate placing a piece\"\"\"\n",
    "    board_copy = [row[:] for row in board.board[:board.height]]\n",
    "    \n",
    "    try:\n",
    "        y = board.drop_height(piece, x)\n",
    "    except:\n",
    "        return None, 0\n",
    "    \n",
    "    for pos in piece.body:\n",
    "        row_idx = y + pos[1]\n",
    "        col_idx = x + pos[0]\n",
    "        if 0 <= row_idx < board.height and 0 <= col_idx < board.width:\n",
    "            board_copy[row_idx][col_idx] = True\n",
    "    \n",
    "    lines_cleared = sum(1 for row in board_copy if all(row))\n",
    "    \n",
    "    return board_copy, lines_cleared\n",
    "\n",
    "#Agent\n",
    "class EvolutionaryAgent:\n",
    "    def __init__(self):\n",
    "        # Start with current best weights\n",
    "        self.best_weights = {\n",
    "            'aggregate_height': -0.8385,\n",
    "            'max_height': -1.0613,\n",
    "            'holes': -1.1523,\n",
    "            'bumpiness': -0.0481,\n",
    "            'complete_lines': 0.4979,\n",
    "            'wells': -0.5756\n",
    "        }\n",
    "        \n",
    "        self.population = []\n",
    "        self.generation = 0\n",
    "        self.best_fitness = 0\n",
    "        self.best_pieces = 0\n",
    "        self.best_rows = 0\n",
    "        self.evaluations = 0\n",
    "        \n",
    "        # Try to load checkpoint\n",
    "        if os.path.exists(CHECKPOINT_PATH):\n",
    "            self.load_checkpoint()\n",
    "            print(f\"Loaded evolution: Gen {self.generation}, Best fitness: {self.best_fitness:.0f}\")\n",
    "            print(f\"Best performance: {self.best_pieces} pieces, {self.best_rows} rows\")\n",
    "        else:\n",
    "            print(\"Starting evolution from scratch\")\n",
    "            self.initialize_population()\n",
    "    \n",
    "    def initialize_population(self):\n",
    "        \"\"\"Create initial population around best known weights\"\"\"\n",
    "        self.population = []\n",
    "        \n",
    "        # Add the best known weights\n",
    "        self.population.append({\n",
    "            'weights': self.best_weights.copy(),\n",
    "            'fitness': 0,\n",
    "            'games_played': 0\n",
    "        })\n",
    "        \n",
    "        # Create variations\n",
    "        for _ in range(POPULATION_SIZE - 1):\n",
    "            weights = {}\n",
    "            for key, value in self.best_weights.items():\n",
    "                # Add random variation\n",
    "                noise = random.gauss(0, 0.3)\n",
    "                weights[key] = value + noise\n",
    "            \n",
    "            self.population.append({\n",
    "                'weights': weights,\n",
    "                'fitness': 0,\n",
    "                'games_played': 0\n",
    "            })\n",
    "    \n",
    "    def evaluate_position(self, board, piece, x, weights):\n",
    "        \"\"\"Evaluate a position using given weights\"\"\"\n",
    "        simulated_board, lines_cleared = simulate_placement(board, piece, x)\n",
    "        \n",
    "        if simulated_board is None:\n",
    "            return -999999\n",
    "        \n",
    "        features = evaluate_board(simulated_board)\n",
    "        \n",
    "        score = 0\n",
    "        for feature, value in features.items():\n",
    "            score += weights[feature] * value\n",
    "        \n",
    "        return score\n",
    "    \n",
    "    def get_best_move(self, board, piece, weights=None):\n",
    "        \"\"\"Get best move using given weights\"\"\"\n",
    "        if weights is None:\n",
    "            weights = self.best_weights\n",
    "        \n",
    "        best_x = -1\n",
    "        best_piece = None\n",
    "        best_score = -999999\n",
    "        \n",
    "        current_piece = piece\n",
    "        for rotation in range(4):\n",
    "            for x in range(board.width):\n",
    "                if x + len(current_piece.skirt) > board.width:\n",
    "                    continue\n",
    "                \n",
    "                try:\n",
    "                    score = self.evaluate_position(board, current_piece, x, weights)\n",
    "                    if score > best_score:\n",
    "                        best_score = score\n",
    "                        best_x = x\n",
    "                        best_piece = current_piece\n",
    "                except:\n",
    "                    pass\n",
    "            \n",
    "            current_piece = current_piece.get_next_rotation()\n",
    "        \n",
    "        if best_x == -1:\n",
    "            best_x = 0\n",
    "            best_piece = piece\n",
    "        \n",
    "        return best_x, best_piece\n",
    "    \n",
    "    def evolve_generation(self):\n",
    "        \"\"\"Evolve to next generation\"\"\"\n",
    "        # Sort by fitness\n",
    "        self.population.sort(key=lambda x: x['fitness'], reverse=True)\n",
    "        \n",
    "        # Update best\n",
    "        if self.population[0]['fitness'] > self.best_fitness:\n",
    "            self.best_fitness = self.population[0]['fitness']\n",
    "            self.best_weights = self.population[0]['weights'].copy()\n",
    "        \n",
    "        # Keep elite\n",
    "        new_population = self.population[:ELITE_SIZE]\n",
    "        \n",
    "        # Create offspring\n",
    "        while len(new_population) < POPULATION_SIZE:\n",
    "            # Select parents from elite\n",
    "            parent1 = random.choice(self.population[:ELITE_SIZE])\n",
    "            parent2 = random.choice(self.population[:ELITE_SIZE])\n",
    "            \n",
    "            # Crossover\n",
    "            child_weights = {}\n",
    "            for key in self.best_weights.keys():\n",
    "                if random.random() < 0.5:\n",
    "                    child_weights[key] = parent1['weights'][key]\n",
    "                else:\n",
    "                    child_weights[key] = parent2['weights'][key]\n",
    "                \n",
    "                # Mutation\n",
    "                if random.random() < MUTATION_RATE:\n",
    "                    noise = random.gauss(0, MUTATION_STRENGTH)\n",
    "                    child_weights[key] += noise\n",
    "            \n",
    "            new_population.append({\n",
    "                'weights': child_weights,\n",
    "                'fitness': 0,\n",
    "                'games_played': 0\n",
    "            })\n",
    "        \n",
    "        self.population = new_population\n",
    "        self.generation += 1\n",
    "        \n",
    "        # Save checkpoint\n",
    "        self.save_checkpoint()\n",
    "    \n",
    "    def save_checkpoint(self):\n",
    "        \"\"\"Save evolution state\"\"\"\n",
    "        data = {\n",
    "            'best_weights': self.best_weights,\n",
    "            'population': self.population,\n",
    "            'generation': self.generation,\n",
    "            'best_fitness': self.best_fitness,\n",
    "            'best_pieces': self.best_pieces,\n",
    "            'best_rows': self.best_rows,\n",
    "            'evaluations': self.evaluations\n",
    "        }\n",
    "        with open(CHECKPOINT_PATH, 'wb') as f:\n",
    "            pickle.dump(data, f)\n",
    "    \n",
    "    def load_checkpoint(self):\n",
    "        \"\"\"Load evolution state\"\"\"\n",
    "        with open(CHECKPOINT_PATH, 'rb') as f:\n",
    "            data = pickle.load(f)\n",
    "        self.best_weights = data['best_weights']\n",
    "        self.population = data.get('population', [])\n",
    "        self.generation = data.get('generation', 0)\n",
    "        self.best_fitness = data.get('best_fitness', 0)\n",
    "        self.best_pieces = data.get('best_pieces', 0)\n",
    "        self.best_rows = data.get('best_rows', 0)\n",
    "        self.evaluations = data.get('evaluations', 0)\n",
    "        \n",
    "        if not self.population:\n",
    "            self.initialize_population()\n",
    "\n",
    "# ============================================================================\n",
    "# MAIN AI CLASS\n",
    "# ============================================================================\n",
    "_agent = None\n",
    "_first_game = True\n",
    "_current_individual = 0\n",
    "_current_game_in_eval = 0\n",
    "\n",
    "class CUSTOM_AI_MODEL:\n",
    "    def __init__(self):\n",
    "        global _agent, _first_game\n",
    "        \n",
    "        if _agent is None:\n",
    "            _agent = EvolutionaryAgent()\n",
    "        \n",
    "        self.agent = _agent\n",
    "        self.pieces_count = 0\n",
    "        self.rows_count = 0\n",
    "        self.game_over = False\n",
    "        \n",
    "        if TRAINING_MODE and self.agent.population:\n",
    "            self.current_weights = self.agent.population[_current_individual]['weights']\n",
    "        else:\n",
    "            self.current_weights = self.agent.best_weights\n",
    "        \n",
    "        if _first_game:\n",
    "            _first_game = False\n",
    "            if TRAINING_MODE:\n",
    "                print(\"\\n\" + \"=\"*80)\n",
    "                print(\"EVOLUTIONARY TETRIS AI - TRAINING MODE\")\n",
    "                print(\"=\"*80)\n",
    "                print(f\"Generation: {self.agent.generation}\")\n",
    "                print(f\"Population size: {POPULATION_SIZE}\")\n",
    "                print(f\"Games per individual: {GAMES_PER_INDIVIDUAL}\")\n",
    "                print(f\"Games per generation: {POPULATION_SIZE * GAMES_PER_INDIVIDUAL}\")\n",
    "                print(f\"Best fitness so far: {self.agent.best_fitness:.0f}\")\n",
    "                print(f\"Best performance: {self.agent.best_pieces}p/{self.agent.best_rows}r\")\n",
    "                print(f\"Checkpoint: {CHECKPOINT_PATH}\")\n",
    "                print(f\"Log file: {LOG_FILE}\")\n",
    "                print(\"=\"*80 + \"\\n\")\n",
    "                \n",
    "                # Write initial log entry\n",
    "                with open(LOG_FILE, 'a') as f:\n",
    "                    f.write(f\"\\n{'='*80}\\n\")\n",
    "                    f.write(f\"Training resumed at generation {self.agent.generation}\\n\")\n",
    "                    f.write(f\"Best so far: {self.agent.best_pieces}p/{self.agent.best_rows}r\\n\")\n",
    "                    f.write(f\"{'='*80}\\n\\n\")\n",
    "                    f.flush()\n",
    "            else:\n",
    "                print(\"\\n\" + \"=\"*80)\n",
    "                print(\"EVOLUTIONARY TETRIS AI - PLAY MODE\")\n",
    "                print(\"=\"*80)\n",
    "                print(\"Using best evolved weights:\")\n",
    "                for key, value in self.agent.best_weights.items():\n",
    "                    print(f\"  {key}: {value:.3f}\")\n",
    "                print(\"=\"*80 + \"\\n\")\n",
    "    \n",
    "    def get_best_move(self, board, piece, depth=1):\n",
    "        \"\"\"Main method called by game.py\"\"\"\n",
    "        x, selected_piece = self.agent.get_best_move(board, piece, self.current_weights)\n",
    "        self.pieces_count += 1\n",
    "        return x, selected_piece\n",
    "    \n",
    "    def __del__(self):\n",
    "        \"\"\"Called when game ends - handle evolution\"\"\"\n",
    "        if not hasattr(self, 'pieces_count') or not TRAINING_MODE:\n",
    "            return\n",
    "        \n",
    "        if self.pieces_count > 0 and not self.game_over:\n",
    "            self.game_over = True\n",
    "            self._continue_evolution()\n",
    "    \n",
    "    def _continue_evolution(self):\n",
    "        \"\"\"Continue evolutionary training\"\"\"\n",
    "        global _current_individual, _current_game_in_eval\n",
    "        from game import Game\n",
    "        \n",
    "        try:\n",
    "            while True:\n",
    "                # Run a game\n",
    "                g = Game(\"student\")\n",
    "                pieces, rows = g.run_no_visual()\n",
    "                \n",
    "                # Evaluate current individual\n",
    "                individual = self.agent.population[_current_individual]\n",
    "                \n",
    "                # Update fitness - HEAVILY PRIORITIZE ROWS WITH HARSH PENALTIES\n",
    "                fitness = pieces + rows * 100  # Rows worth 100 pieces\n",
    "                \n",
    "                # HARSH PENALTIES for poor performance\n",
    "                if pieces < 1000:\n",
    "                    fitness *= 0.5  # 50% penalty for dying too early\n",
    "                if rows < 100:\n",
    "                    fitness *= 0.7  # 30% penalty for not clearing enough rows\n",
    "                if rows == 0:\n",
    "                    fitness *= 0.3  # 70% penalty for clearing NO rows\n",
    "                \n",
    "                # BONUSES for excellent performance\n",
    "                if pieces > 50000:\n",
    "                    fitness *= 1.5  # 50% bonus for very long games\n",
    "                if rows > 20000:\n",
    "                    fitness *= 2.0  # 100% bonus for clearing tons of rows\n",
    "                \n",
    "                individual['fitness'] += fitness\n",
    "                individual['games_played'] += 1\n",
    "                \n",
    "                _current_game_in_eval += 1\n",
    "                self.agent.evaluations += 1\n",
    "                \n",
    "                # Track best and save immediately when improved\n",
    "                if pieces > self.agent.best_pieces:\n",
    "                    self.agent.best_pieces = pieces\n",
    "                    print(f\"*** NEW BEST PIECES: {pieces} ***\")\n",
    "                    self.agent.save_checkpoint()\n",
    "                    \n",
    "                if rows > self.agent.best_rows:\n",
    "                    self.agent.best_rows = rows\n",
    "                    print(f\"*** NEW BEST ROWS: {rows} ***\")\n",
    "                    self.agent.save_checkpoint()\n",
    "                \n",
    "                # Also save if overall fitness is best\n",
    "                current_fitness = pieces + rows * 100\n",
    "                if current_fitness > self.agent.best_fitness:\n",
    "                    print(f\"*** NEW BEST FITNESS: {current_fitness:.0f} ***\")\n",
    "                    self.agent.best_fitness = current_fitness\n",
    "                    self.agent.save_checkpoint()\n",
    "                \n",
    "                print(f\"Gen {self.agent.generation} | \"\n",
    "                      f\"Ind {_current_individual + 1}/{POPULATION_SIZE} | \"\n",
    "                      f\"Game {_current_game_in_eval}/{GAMES_PER_INDIVIDUAL} | \"\n",
    "                      f\"Pieces: {pieces:5d} | Rows: {rows:5d} | \"\n",
    "                      f\"Fitness: {fitness:7.0f}\")\n",
    "                \n",
    "                # Move to next individual or generation\n",
    "                if _current_game_in_eval >= GAMES_PER_INDIVIDUAL:\n",
    "                    individual['fitness'] /= GAMES_PER_INDIVIDUAL\n",
    "                    \n",
    "                    _current_game_in_eval = 0\n",
    "                    _current_individual += 1\n",
    "                    \n",
    "                    if _current_individual >= POPULATION_SIZE:\n",
    "                        # Generation complete\n",
    "                        _current_individual = 0\n",
    "                        \n",
    "                        avg_fitness = sum(ind['fitness'] for ind in self.agent.population) / POPULATION_SIZE\n",
    "                        best_ind = max(self.agent.population, key=lambda x: x['fitness'])\n",
    "                        \n",
    "                        log_msg = (f\"\\n{'='*80}\\n\"\n",
    "                                 f\"GENERATION {self.agent.generation} COMPLETE\\n\"\n",
    "                                 f\"{'='*80}\\n\"\n",
    "                                 f\"Average fitness: {avg_fitness:.0f}\\n\"\n",
    "                                 f\"Best fitness this gen: {best_ind['fitness']:.0f}\\n\"\n",
    "                                 f\"All-time best fitness: {self.agent.best_fitness:.0f}\\n\"\n",
    "                                 f\"All-time best pieces: {self.agent.best_pieces:,}\\n\"\n",
    "                                 f\"All-time best rows: {self.agent.best_rows:,}\\n\"\n",
    "                                 f\"Total evaluations: {self.agent.evaluations}\\n\"\n",
    "                                 f\"{'='*80}\\n\")\n",
    "                        print(log_msg)\n",
    "                        \n",
    "                        # Write to log file with flush\n",
    "                        with open(LOG_FILE, 'a') as f:\n",
    "                            f.write(log_msg)\n",
    "                            f.flush()\n",
    "                        \n",
    "                        # Evolve to next generation\n",
    "                        self.agent.evolve_generation()\n",
    "                        \n",
    "                        # Save after evolution\n",
    "                        self.agent.save_checkpoint()\n",
    "                        \n",
    "                        print(f\"Evolved to generation {self.agent.generation}\")\n",
    "                        print(f\"Checkpoint saved: {CHECKPOINT_PATH}\")\n",
    "                        print(f\"Log saved: {LOG_FILE}\\n\")\n",
    "                \n",
    "        except KeyboardInterrupt:\n",
    "            print(\"\\n\" + \"=\"*80)\n",
    "            print(\"Evolution stopped by user\")\n",
    "            print(f\"Generation: {self.agent.generation}\")\n",
    "            print(f\"Best fitness: {self.agent.best_fitness:.0f}\")\n",
    "            print(f\"Best performance: {self.agent.best_pieces}p/{self.agent.best_rows}r\")\n",
    "            print(\"=\"*80)\n",
    "            self.agent.save_checkpoint()\n",
    "            import sys\n",
    "            sys.exit(0)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 7. Present\n",
    "In a customer faceing Document provide summery of finding and detail approach taken\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The model employs a genetic algorithm to evolve certain heuristic weights for a board evaluation function. For each piece placement decision, the algorithm simulates all possible positions and rotations in a game of Tetris, it then evaluates each resulting board state using six weighted features (aggregate height, maximum height, holes, bumpiness, complete lines, and wells), and selects the placement with the highest weighted score. The weights are evolved over generations using a population of 20 individuals, where each individual represents a unique weight tested across 5 games to compute average fitness. After evaluation, the top 4 individuals are preserved, while the remaining population is generated through crossover and mutation of the elite parents. This process iterates across generations, with the target network updating after each generation to incorporate the best-performing weights."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 8. Launch the Model System\n",
    "Define your production run code, This should be self susficent and require only your model pramaters \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  },
  "toc-autonumbering": false,
  "toc-showmarkdowntxt": false
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
